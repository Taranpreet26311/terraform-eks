{
  "version": 4,
  "terraform_version": "0.13.2",
  "serial": 43,
  "lineage": "71d8b1cb-cb9f-9b26-fd97-af1b83ddcb24",
  "outputs": {
    "config_map_aws_auth": {
      "value": "\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: aws-auth\n  namespace: kube-system\ndata:\n  mapRoles: |\n    - rolearn: arn:aws:iam::158529055655:role/terraform-eks-cluster-node\n      username: system:node:{{EC2PrivateDNSName}}\n      groups:\n        - system:bootstrappers\n        - system:nodes\n",
      "type": "string"
    },
    "kubeconfig": {
      "value": "\n\napiVersion: v1\nclusters:\n- cluster:\n    server: https://CDD6E8DD87D705CE6C5703396E6F6D73.gr7.us-west-1.eks.amazonaws.com\n    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeE1ERXhPVEUyTlRjek1Wb1hEVE14TURFeE56RTJOVGN6TVZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTXNGCjJVVlpsM3RCRXFQZTh1V0JLcnIyS3A5NVlmNkRXNGFCL041UmgzL2lXMXFYSldtZG0rMThVVkJlVzVuMnIwc0sKRW94RjN0U2R4VkpDYnl3NkRyVWRCNWRtTlFBbW1zNjdTN0UvVElFcjJ3WFdFYVorMVJKQzlnbXNtN1k3SUVNTwpVcHZjaURaSjRhZnBoVnljczQvN0xvVTA0TkpyV080SzIxVWM4MW5DMi9Ya1JMVFMvYlVCUmpiTm8vZ1Y4K3M5CnkyUXg5ak1yWmdSM0pkTGc1R2ZKdVJ2ZnhVWFFVT0xGdnpKSnAyTkxVQnptelhWZnczNGhDUmxhVTRydGlWaGcKbTFJRmd5dzZlbUE1dUE5Ull2NzgxaUI5VXhQdWViR24zQjJTTzlJUjVEV0lEU2pjMkdRWWdjSVp0M1B1ZWtPVQpKK0RSQ3ZNT1B4VFNXbVBCekhNQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFBWmRSL0Z5c0JUdGNqZWtoblc2Rk9IeWQ2YzUKWkp6TmdXRndRaU9JUHE1U013L3FWQm9HRFFPMWp6NzUvcjVHWkxvWWtTbFJtWSttU3B1Vk1LUnhyd0hEdXRVZQpuM0h4Mmw1bDN0cVg0M1VXN1NiZnZjSTdPb05lSGFWbVZTczIyYzZUU1RobUR5SGV1YktTMldPWkVZU1pOMlFmCjU4b0RxWkU5RUhHVzdjNzRmS0JnVVhBQldPWXJBQWtwRXV4Q0pSQVNvdUMxbmdrOGlsNXIxVEMwcUpQSG1OYWMKT3JCWFBkVGI3R1R4dENMWTE4dlEwZW9kRVBIcExXVHVUeHFiUzloT0NtdzJ4bWk3aFBsUHhMWkhhWWduRk5BdgpKV2M4L2dTMU5xM3lkUjdHVEx1bGxWVXo2ZXg2WEhLYlBQQ21md0ppcVdxUVBHM24yUGlXbkJ6Y3cxMD0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=\n  name: kubernetes\ncontexts:\n- context:\n    cluster: kubernetes\n    user: aws\n  name: aws\ncurrent-context: aws\nkind: Config\npreferences: {}\nusers:\n- name: aws\n  user:\n    exec:\n      apiVersion: client.authentication.k8s.io/v1alpha1\n      command: aws-iam-authenticator\n      args:\n        - \"token\"\n        - \"-i\"\n        - \"terraform-eks-cluster\"\n",
      "type": "string"
    }
  },
  "resources": [
    {
      "mode": "data",
      "type": "aws_availability_zones",
      "name": "available",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "all_availability_zones": null,
            "exclude_names": null,
            "exclude_zone_ids": null,
            "filter": null,
            "group_names": [
              "us-west-1"
            ],
            "id": "us-west-1",
            "names": [
              "us-west-1b",
              "us-west-1c"
            ],
            "state": null,
            "zone_ids": [
              "usw1-az3",
              "usw1-az1"
            ]
          }
        }
      ]
    },
    {
      "mode": "data",
      "type": "http",
      "name": "workstation-external-ip",
      "provider": "provider[\"registry.terraform.io/hashicorp/http\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "body": "49.36.148.172\n",
            "id": "http://ipv4.icanhazip.com",
            "request_headers": null,
            "response_headers": {
              "Access-Control-Allow-Methods": "GET",
              "Access-Control-Allow-Origin": "*",
              "Content-Length": "14",
              "Content-Type": "text/plain; charset=UTF-8",
              "Date": "Tue, 19 Jan 2021 17:37:28 GMT",
              "Server": "nginx",
              "X-Donation": "This site is expensive to run. You can donate BTC to 3LSp89k9qnMJBpV7AUNF3M2Eo1vatpkYpm",
              "X-Duck": "ðŸ¦†",
              "X-Node": "icanhazip-dfw-1",
              "X-Rtfm": "Learn about this site at http://bit.ly/icanhazip-faq and do not abuse the service."
            },
            "url": "http://ipv4.icanhazip.com"
          }
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_eks_cluster",
      "name": "demo",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:eks:us-west-1:158529055655:cluster/terraform-eks-cluster",
            "certificate_authority": [
              {
                "data": "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeE1ERXhPVEUyTlRjek1Wb1hEVE14TURFeE56RTJOVGN6TVZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTXNGCjJVVlpsM3RCRXFQZTh1V0JLcnIyS3A5NVlmNkRXNGFCL041UmgzL2lXMXFYSldtZG0rMThVVkJlVzVuMnIwc0sKRW94RjN0U2R4VkpDYnl3NkRyVWRCNWRtTlFBbW1zNjdTN0UvVElFcjJ3WFdFYVorMVJKQzlnbXNtN1k3SUVNTwpVcHZjaURaSjRhZnBoVnljczQvN0xvVTA0TkpyV080SzIxVWM4MW5DMi9Ya1JMVFMvYlVCUmpiTm8vZ1Y4K3M5CnkyUXg5ak1yWmdSM0pkTGc1R2ZKdVJ2ZnhVWFFVT0xGdnpKSnAyTkxVQnptelhWZnczNGhDUmxhVTRydGlWaGcKbTFJRmd5dzZlbUE1dUE5Ull2NzgxaUI5VXhQdWViR24zQjJTTzlJUjVEV0lEU2pjMkdRWWdjSVp0M1B1ZWtPVQpKK0RSQ3ZNT1B4VFNXbVBCekhNQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFBWmRSL0Z5c0JUdGNqZWtoblc2Rk9IeWQ2YzUKWkp6TmdXRndRaU9JUHE1U013L3FWQm9HRFFPMWp6NzUvcjVHWkxvWWtTbFJtWSttU3B1Vk1LUnhyd0hEdXRVZQpuM0h4Mmw1bDN0cVg0M1VXN1NiZnZjSTdPb05lSGFWbVZTczIyYzZUU1RobUR5SGV1YktTMldPWkVZU1pOMlFmCjU4b0RxWkU5RUhHVzdjNzRmS0JnVVhBQldPWXJBQWtwRXV4Q0pSQVNvdUMxbmdrOGlsNXIxVEMwcUpQSG1OYWMKT3JCWFBkVGI3R1R4dENMWTE4dlEwZW9kRVBIcExXVHVUeHFiUzloT0NtdzJ4bWk3aFBsUHhMWkhhWWduRk5BdgpKV2M4L2dTMU5xM3lkUjdHVEx1bGxWVXo2ZXg2WEhLYlBQQ21md0ppcVdxUVBHM24yUGlXbkJ6Y3cxMD0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo="
              }
            ],
            "created_at": "2021-01-19 16:48:20.743 +0000 UTC",
            "enabled_cluster_log_types": [],
            "encryption_config": [],
            "endpoint": "https://CDD6E8DD87D705CE6C5703396E6F6D73.gr7.us-west-1.eks.amazonaws.com",
            "id": "terraform-eks-cluster",
            "identity": [
              {
                "oidc": [
                  {
                    "issuer": "https://oidc.eks.us-west-1.amazonaws.com/id/CDD6E8DD87D705CE6C5703396E6F6D73"
                  }
                ]
              }
            ],
            "kubernetes_network_config": [
              {
                "service_ipv4_cidr": "172.20.0.0/16"
              }
            ],
            "name": "terraform-eks-cluster",
            "platform_version": "eks.3",
            "role_arn": "arn:aws:iam::158529055655:role/terraform-eks-cluster-cluster",
            "status": "ACTIVE",
            "tags": {},
            "timeouts": null,
            "version": "1.18",
            "vpc_config": [
              {
                "cluster_security_group_id": "sg-0432b62c7a1e72db6",
                "endpoint_private_access": false,
                "endpoint_public_access": true,
                "public_access_cidrs": [
                  "0.0.0.0/0"
                ],
                "security_group_ids": [
                  "sg-0e51ebe02dabad70c"
                ],
                "subnet_ids": [
                  "subnet-0a910aee00a11a8f4",
                  "subnet-0f75bccc8da94adea"
                ],
                "vpc_id": "vpc-0ad85369262090d98"
              }
            ]
          },
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjoxODAwMDAwMDAwMDAwLCJkZWxldGUiOjkwMDAwMDAwMDAwMCwidXBkYXRlIjozNjAwMDAwMDAwMDAwfX0=",
          "dependencies": [
            "aws_iam_role.demo-cluster",
            "aws_iam_role_policy_attachment.demo-cluster-AmazonEKSClusterPolicy",
            "aws_iam_role_policy_attachment.demo-cluster-AmazonEKSVPCResourceController",
            "aws_security_group.demo-cluster",
            "aws_subnet.demo",
            "aws_vpc.demo",
            "data.aws_availability_zones.available"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_eks_node_group",
      "name": "demo",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "ami_type": "AL2_x86_64",
            "arn": "arn:aws:eks:us-west-1:158529055655:nodegroup/terraform-eks-cluster/demo/9cbb8dd3-ffe6-d842-ab80-5b5987b10f27",
            "capacity_type": "ON_DEMAND",
            "cluster_name": "terraform-eks-cluster",
            "disk_size": 20,
            "force_update_version": null,
            "id": "terraform-eks-cluster:demo",
            "instance_types": [
              "t3.large"
            ],
            "labels": {},
            "launch_template": [],
            "node_group_name": "demo",
            "node_role_arn": "arn:aws:iam::158529055655:role/terraform-eks-cluster-node",
            "release_version": "1.18.9-20210112",
            "remote_access": [],
            "resources": [
              {
                "autoscaling_groups": [
                  {
                    "name": "eks-9cbb8dd3-ffe6-d842-ab80-5b5987b10f27"
                  }
                ],
                "remote_access_security_group_id": ""
              }
            ],
            "scaling_config": [
              {
                "desired_size": 3,
                "max_size": 5,
                "min_size": 3
              }
            ],
            "status": "ACTIVE",
            "subnet_ids": [
              "subnet-0a910aee00a11a8f4",
              "subnet-0f75bccc8da94adea"
            ],
            "tags": {},
            "timeouts": null,
            "version": "1.18"
          },
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjozNjAwMDAwMDAwMDAwLCJkZWxldGUiOjM2MDAwMDAwMDAwMDAsInVwZGF0ZSI6MzYwMDAwMDAwMDAwMH19",
          "dependencies": [
            "aws_eks_cluster.demo",
            "aws_iam_role.demo-node",
            "aws_iam_role_policy_attachment.demo-node-AmazonEC2ContainerRegistryReadOnly",
            "aws_iam_role_policy_attachment.demo-node-AmazonEKSWorkerNodePolicy",
            "aws_iam_role_policy_attachment.demo-node-AmazonEKS_CNI_Policy",
            "aws_subnet.demo"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role",
      "name": "demo-cluster",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:iam::158529055655:role/terraform-eks-cluster-cluster",
            "assume_role_policy": "{\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Principal\":{\"Service\":\"eks.amazonaws.com\"},\"Action\":\"sts:AssumeRole\"}]}",
            "create_date": "2021-01-19T16:47:56Z",
            "description": "",
            "force_detach_policies": false,
            "id": "terraform-eks-cluster-cluster",
            "max_session_duration": 3600,
            "name": "terraform-eks-cluster-cluster",
            "name_prefix": null,
            "path": "/",
            "permissions_boundary": null,
            "tags": {},
            "unique_id": "AROASJ2IRNOT5Q3H26UVO"
          },
          "private": "bnVsbA=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role",
      "name": "demo-node",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:iam::158529055655:role/terraform-eks-cluster-node",
            "assume_role_policy": "{\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Principal\":{\"Service\":\"ec2.amazonaws.com\"},\"Action\":\"sts:AssumeRole\"}]}",
            "create_date": "2021-01-19T16:47:56Z",
            "description": "",
            "force_detach_policies": false,
            "id": "terraform-eks-cluster-node",
            "max_session_duration": 3600,
            "name": "terraform-eks-cluster-node",
            "name_prefix": null,
            "path": "/",
            "permissions_boundary": null,
            "tags": {},
            "unique_id": "AROASJ2IRNOTWD4GL2ARC"
          },
          "private": "bnVsbA=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role_policy_attachment",
      "name": "demo-cluster-AmazonEKSClusterPolicy",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "terraform-eks-cluster-cluster-20210119164759140500000004",
            "policy_arn": "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy",
            "role": "terraform-eks-cluster-cluster"
          },
          "private": "bnVsbA==",
          "dependencies": [
            "aws_iam_role.demo-cluster"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role_policy_attachment",
      "name": "demo-cluster-AmazonEKSVPCResourceController",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "terraform-eks-cluster-cluster-20210119164759159600000005",
            "policy_arn": "arn:aws:iam::aws:policy/AmazonEKSVPCResourceController",
            "role": "terraform-eks-cluster-cluster"
          },
          "private": "bnVsbA==",
          "dependencies": [
            "aws_iam_role.demo-cluster"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role_policy_attachment",
      "name": "demo-node-AmazonEC2ContainerRegistryReadOnly",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "terraform-eks-cluster-node-20210119164759070600000003",
            "policy_arn": "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly",
            "role": "terraform-eks-cluster-node"
          },
          "private": "bnVsbA==",
          "dependencies": [
            "aws_iam_role.demo-node"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role_policy_attachment",
      "name": "demo-node-AmazonEKSWorkerNodePolicy",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "terraform-eks-cluster-node-20210119164759020400000001",
            "policy_arn": "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy",
            "role": "terraform-eks-cluster-node"
          },
          "private": "bnVsbA==",
          "dependencies": [
            "aws_iam_role.demo-node"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_iam_role_policy_attachment",
      "name": "demo-node-AmazonEKS_CNI_Policy",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "terraform-eks-cluster-node-20210119164759059700000002",
            "policy_arn": "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy",
            "role": "terraform-eks-cluster-node"
          },
          "private": "bnVsbA==",
          "dependencies": [
            "aws_iam_role.demo-node"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_internet_gateway",
      "name": "demo",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:ec2:us-west-1:158529055655:internet-gateway/igw-081f0e0cfdf6a30b7",
            "id": "igw-081f0e0cfdf6a30b7",
            "owner_id": "158529055655",
            "tags": {
              "Name": "terraform-eks-cluster"
            },
            "vpc_id": "vpc-0ad85369262090d98"
          },
          "private": "bnVsbA==",
          "dependencies": [
            "aws_vpc.demo"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_route_table",
      "name": "demo",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "rtb-0fb94320fe00e253c",
            "owner_id": "158529055655",
            "propagating_vgws": [],
            "route": [
              {
                "cidr_block": "0.0.0.0/0",
                "egress_only_gateway_id": "",
                "gateway_id": "igw-081f0e0cfdf6a30b7",
                "instance_id": "",
                "ipv6_cidr_block": "",
                "local_gateway_id": "",
                "nat_gateway_id": "",
                "network_interface_id": "",
                "transit_gateway_id": "",
                "vpc_endpoint_id": "",
                "vpc_peering_connection_id": ""
              }
            ],
            "tags": {},
            "vpc_id": "vpc-0ad85369262090d98"
          },
          "private": "bnVsbA==",
          "dependencies": [
            "aws_internet_gateway.demo",
            "aws_vpc.demo"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_route_table_association",
      "name": "demo",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 0,
          "attributes": {
            "gateway_id": null,
            "id": "rtbassoc-099ef5c008f08044a",
            "route_table_id": "rtb-0fb94320fe00e253c",
            "subnet_id": "subnet-0a910aee00a11a8f4"
          },
          "private": "bnVsbA==",
          "dependencies": [
            "aws_internet_gateway.demo",
            "aws_route_table.demo",
            "aws_subnet.demo",
            "aws_vpc.demo",
            "data.aws_availability_zones.available"
          ]
        },
        {
          "index_key": 1,
          "schema_version": 0,
          "attributes": {
            "gateway_id": null,
            "id": "rtbassoc-0ca3f46cf051d8545",
            "route_table_id": "rtb-0fb94320fe00e253c",
            "subnet_id": "subnet-0f75bccc8da94adea"
          },
          "private": "bnVsbA==",
          "dependencies": [
            "aws_internet_gateway.demo",
            "aws_route_table.demo",
            "aws_subnet.demo",
            "aws_vpc.demo",
            "data.aws_availability_zones.available"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_security_group",
      "name": "demo-cluster",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "arn": "arn:aws:ec2:us-west-1:158529055655:security-group/sg-0e51ebe02dabad70c",
            "description": "Cluster communication with worker nodes",
            "egress": [
              {
                "cidr_blocks": [
                  "0.0.0.0/0"
                ],
                "description": "",
                "from_port": 0,
                "ipv6_cidr_blocks": [],
                "prefix_list_ids": [],
                "protocol": "-1",
                "security_groups": [],
                "self": false,
                "to_port": 0
              }
            ],
            "id": "sg-0e51ebe02dabad70c",
            "ingress": [
              {
                "cidr_blocks": [
                  "49.36.148.172/32"
                ],
                "description": "Allow workstation to communicate with the cluster API Server",
                "from_port": 443,
                "ipv6_cidr_blocks": [],
                "prefix_list_ids": [],
                "protocol": "tcp",
                "security_groups": [],
                "self": false,
                "to_port": 443
              }
            ],
            "name": "terraform-eks-cluster-cluster",
            "name_prefix": "",
            "owner_id": "158529055655",
            "revoke_rules_on_delete": false,
            "tags": {
              "Name": "terraform-eks-cluster"
            },
            "timeouts": null,
            "vpc_id": "vpc-0ad85369262090d98"
          },
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDAsImRlbGV0ZSI6NjAwMDAwMDAwMDAwfSwic2NoZW1hX3ZlcnNpb24iOiIxIn0=",
          "dependencies": [
            "aws_vpc.demo"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_security_group_rule",
      "name": "demo-cluster-ingress-workstation-https",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 2,
          "attributes": {
            "cidr_blocks": [
              "49.36.148.172/32"
            ],
            "description": "Allow workstation to communicate with the cluster API Server",
            "from_port": 443,
            "id": "sgrule-800218623",
            "ipv6_cidr_blocks": [],
            "prefix_list_ids": [],
            "protocol": "tcp",
            "security_group_id": "sg-0e51ebe02dabad70c",
            "self": false,
            "source_security_group_id": null,
            "to_port": 443,
            "type": "ingress"
          },
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjIifQ==",
          "dependencies": [
            "aws_security_group.demo-cluster",
            "aws_vpc.demo",
            "data.http.workstation-external-ip"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_subnet",
      "name": "demo",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 1,
          "attributes": {
            "arn": "arn:aws:ec2:us-west-1:158529055655:subnet/subnet-0a910aee00a11a8f4",
            "assign_ipv6_address_on_creation": false,
            "availability_zone": "us-west-1b",
            "availability_zone_id": "usw1-az3",
            "cidr_block": "10.0.0.0/24",
            "id": "subnet-0a910aee00a11a8f4",
            "ipv6_cidr_block": "",
            "ipv6_cidr_block_association_id": "",
            "map_public_ip_on_launch": true,
            "outpost_arn": "",
            "owner_id": "158529055655",
            "tags": {
              "Name": "terraform-eks-cluster-node",
              "kubernetes.io/cluster/terraform-eks-cluster": "shared"
            },
            "timeouts": null,
            "vpc_id": "vpc-0ad85369262090d98"
          },
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDAsImRlbGV0ZSI6MTIwMDAwMDAwMDAwMH0sInNjaGVtYV92ZXJzaW9uIjoiMSJ9",
          "dependencies": [
            "aws_vpc.demo",
            "data.aws_availability_zones.available"
          ]
        },
        {
          "index_key": 1,
          "schema_version": 1,
          "attributes": {
            "arn": "arn:aws:ec2:us-west-1:158529055655:subnet/subnet-0f75bccc8da94adea",
            "assign_ipv6_address_on_creation": false,
            "availability_zone": "us-west-1c",
            "availability_zone_id": "usw1-az1",
            "cidr_block": "10.0.1.0/24",
            "id": "subnet-0f75bccc8da94adea",
            "ipv6_cidr_block": "",
            "ipv6_cidr_block_association_id": "",
            "map_public_ip_on_launch": true,
            "outpost_arn": "",
            "owner_id": "158529055655",
            "tags": {
              "Name": "terraform-eks-cluster-node",
              "kubernetes.io/cluster/terraform-eks-cluster": "shared"
            },
            "timeouts": null,
            "vpc_id": "vpc-0ad85369262090d98"
          },
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDAsImRlbGV0ZSI6MTIwMDAwMDAwMDAwMH0sInNjaGVtYV92ZXJzaW9uIjoiMSJ9",
          "dependencies": [
            "aws_vpc.demo",
            "data.aws_availability_zones.available"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "aws_vpc",
      "name": "demo",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "arn": "arn:aws:ec2:us-west-1:158529055655:vpc/vpc-0ad85369262090d98",
            "assign_generated_ipv6_cidr_block": false,
            "cidr_block": "10.0.0.0/16",
            "default_network_acl_id": "acl-0e423c1e54233c139",
            "default_route_table_id": "rtb-0c4556e484947eb5e",
            "default_security_group_id": "sg-03b02142a278916a1",
            "dhcp_options_id": "dopt-cd00f5ab",
            "enable_classiclink": false,
            "enable_classiclink_dns_support": false,
            "enable_dns_hostnames": false,
            "enable_dns_support": true,
            "id": "vpc-0ad85369262090d98",
            "instance_tenancy": "default",
            "ipv6_association_id": "",
            "ipv6_cidr_block": "",
            "main_route_table_id": "rtb-0c4556e484947eb5e",
            "owner_id": "158529055655",
            "tags": {
              "Name": "terraform-eks-cluster-node",
              "kubernetes.io/cluster/terraform-eks-cluster": "shared"
            }
          },
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "consul",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "./helm/consul-helm/",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "consul",
            "keyring": null,
            "lint": false,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "1.9.1",
                "chart": "consul",
                "name": "consul",
                "namespace": "default",
                "revision": 1,
                "values": "{\"client\":{\"affinity\":{},\"annotations\":null,\"dataDirectoryHostPath\":null,\"dnsPolicy\":null,\"enabled\":true,\"exposeGossipPorts\":false,\"extraConfig\":\"{}\\n\",\"extraEnvironmentVars\":{},\"extraVolumes\":[],\"grpc\":true,\"hostNetwork\":false,\"image\":null,\"join\":null,\"nodeSelector\":null,\"priorityClassName\":\"\",\"resources\":{\"limits\":{\"cpu\":\"100m\",\"memory\":\"100Mi\"},\"requests\":{\"cpu\":\"100m\",\"memory\":\"100Mi\"}},\"securityContext\":{\"fsGroup\":1000,\"runAsGroup\":1000,\"runAsNonRoot\":true,\"runAsUser\":100},\"snapshotAgent\":{\"caCert\":null,\"configSecret\":{\"secretKey\":null,\"secretName\":null},\"enabled\":false,\"replicas\":2,\"resources\":{\"limits\":{\"cpu\":\"50m\",\"memory\":\"50Mi\"},\"requests\":{\"cpu\":\"50m\",\"memory\":\"50Mi\"}}},\"tolerations\":\"\",\"updateStrategy\":null},\"connectInject\":{\"aclBindingRuleSelector\":\"serviceaccount.name!=default\",\"aclInjectToken\":{\"secretKey\":null,\"secretName\":null},\"affinity\":null,\"centralConfig\":{\"defaultProtocol\":null,\"enabled\":true,\"proxyDefaults\":\"{}\\n\"},\"certs\":{\"caBundle\":\"\",\"certName\":\"tls.crt\",\"keyName\":\"tls.key\",\"secretName\":null},\"consulNamespaces\":{\"consulDestinationNamespace\":\"default\",\"mirroringK8S\":false,\"mirroringK8SPrefix\":\"\"},\"default\":false,\"enabled\":true,\"envoyExtraArgs\":null,\"healthChecks\":{\"enabled\":true,\"reconcilePeriod\":\"1m\"},\"image\":null,\"imageConsul\":null,\"initContainer\":{\"resources\":{\"limits\":{\"cpu\":\"50m\",\"memory\":\"150Mi\"},\"requests\":{\"cpu\":\"50m\",\"memory\":\"25Mi\"}}},\"k8sAllowNamespaces\":[\"*\"],\"k8sDenyNamespaces\":[],\"logLevel\":\"info\",\"namespaceSelector\":null,\"nodeSelector\":null,\"overrideAuthMethodName\":\"\",\"priorityClassName\":\"\",\"resources\":{\"limits\":{\"cpu\":\"50m\",\"memory\":\"50Mi\"},\"requests\":{\"cpu\":\"50m\",\"memory\":\"50Mi\"}},\"sidecarProxy\":{\"resources\":{\"limits\":{\"cpu\":null,\"memory\":null},\"requests\":{\"cpu\":null,\"memory\":null}}},\"tolerations\":null},\"controller\":{\"affinity\":null,\"enabled\":false,\"logLevel\":\"info\",\"nodeSelector\":null,\"priorityClassName\":\"\",\"replicas\":1,\"resources\":{\"limits\":{\"cpu\":\"100m\",\"memory\":\"50Mi\"},\"requests\":{\"cpu\":\"100m\",\"memory\":\"50Mi\"}},\"tolerations\":null},\"dns\":{\"additionalSpec\":null,\"annotations\":null,\"clusterIP\":null,\"enabled\":\"-\",\"type\":\"ClusterIP\"},\"externalServers\":{\"enabled\":false,\"hosts\":[],\"httpsPort\":8501,\"k8sAuthMethodHost\":null,\"tlsServerName\":null,\"useSystemRoots\":false},\"global\":{\"acls\":{\"bootstrapToken\":{\"secretKey\":null,\"secretName\":null},\"createReplicationToken\":false,\"manageSystemACLs\":false,\"replicationToken\":{\"secretKey\":null,\"secretName\":null}},\"datacenter\":\"dc1\",\"domain\":\"consul\",\"enableConsulNamespaces\":false,\"enablePodSecurityPolicies\":false,\"enabled\":true,\"federation\":{\"createFederationSecret\":false,\"enabled\":false},\"gossipEncryption\":{\"secretKey\":\"\",\"secretName\":\"\"},\"image\":\"hashicorp/consul:1.9.1\",\"imageEnvoy\":\"envoyproxy/envoy-alpine:v1.16.0\",\"imageK8S\":\"hashicorp/consul-k8s:0.22.0\",\"imagePullSecrets\":[],\"lifecycleSidecarContainer\":{\"resources\":{\"limits\":{\"cpu\":\"20m\",\"memory\":\"50Mi\"},\"requests\":{\"cpu\":\"20m\",\"memory\":\"25Mi\"}}},\"name\":null,\"openshift\":{\"enabled\":false},\"tls\":{\"caCert\":{\"secretKey\":null,\"secretName\":null},\"caKey\":{\"secretKey\":null,\"secretName\":null},\"enableAutoEncrypt\":false,\"enabled\":false,\"httpsOnly\":true,\"serverAdditionalDNSSANs\":[],\"serverAdditionalIPSANs\":[],\"verify\":true}},\"ingressGateways\":{\"defaults\":{\"affinity\":\"podAntiAffinity:\\n  requiredDuringSchedulingIgnoredDuringExecution:\\n    - labelSelector:\\n        matchLabels:\\n          app: {{ template \\\"consul.name\\\" . }}\\n          release: \\\"{{ .Release.Name }}\\\"\\n          component: ingress-gateway\\n      topologyKey: kubernetes.io/hostname\\n\",\"annotations\":null,\"consulNamespace\":\"default\",\"initCopyConsulContainer\":{\"resources\":{\"limits\":{\"cpu\":\"50m\",\"memory\":\"150Mi\"},\"requests\":{\"cpu\":\"50m\",\"memory\":\"25Mi\"}}},\"nodeSelector\":null,\"priorityClassName\":\"\",\"replicas\":2,\"resources\":{\"limits\":{\"cpu\":\"100m\",\"memory\":\"100Mi\"},\"requests\":{\"cpu\":\"100m\",\"memory\":\"100Mi\"}},\"service\":{\"additionalSpec\":null,\"annotations\":null,\"ports\":[{\"nodePort\":null,\"port\":8080},{\"nodePort\":null,\"port\":8443}],\"type\":\"ClusterIP\"},\"tolerations\":null},\"enabled\":false,\"gateways\":[{\"name\":\"ingress-gateway\"}]},\"meshGateway\":{\"affinity\":\"podAntiAffinity:\\n  requiredDuringSchedulingIgnoredDuringExecution:\\n    - labelSelector:\\n        matchLabels:\\n          app: {{ template \\\"consul.name\\\" . }}\\n          release: \\\"{{ .Release.Name }}\\\"\\n          component: mesh-gateway\\n      topologyKey: kubernetes.io/hostname\\n\",\"annotations\":null,\"consulServiceName\":\"mesh-gateway\",\"containerPort\":8443,\"dnsPolicy\":null,\"enabled\":false,\"globalMode\":\"local\",\"hostNetwork\":false,\"hostPort\":null,\"initCopyConsulContainer\":{\"resources\":{\"limits\":{\"cpu\":\"50m\",\"memory\":\"150Mi\"},\"requests\":{\"cpu\":\"50m\",\"memory\":\"25Mi\"}}},\"nodeSelector\":null,\"priorityClassName\":\"\",\"replicas\":2,\"resources\":{\"limits\":{\"cpu\":\"100m\",\"memory\":\"100Mi\"},\"requests\":{\"cpu\":\"100m\",\"memory\":\"100Mi\"}},\"service\":{\"additionalSpec\":null,\"annotations\":null,\"enabled\":true,\"nodePort\":null,\"port\":443,\"type\":\"LoadBalancer\"},\"tolerations\":null,\"wanAddress\":{\"port\":443,\"source\":\"Service\",\"static\":\"\"}},\"server\":{\"affinity\":\"podAntiAffinity:\\n  requiredDuringSchedulingIgnoredDuringExecution:\\n    - labelSelector:\\n        matchLabels:\\n          app: {{ template \\\"consul.name\\\" . }}\\n          release: \\\"{{ .Release.Name }}\\\"\\n          component: server\\n      topologyKey: kubernetes.io/hostname\\n\",\"annotations\":null,\"bootstrapExpect\":null,\"connect\":true,\"disruptionBudget\":{\"enabled\":true,\"maxUnavailable\":null},\"enabled\":\"-\",\"enterpriseLicense\":{\"secretKey\":null,\"secretName\":null},\"exposeGossipAndRPCPorts\":false,\"extraConfig\":\"{}\\n\",\"extraEnvironmentVars\":{},\"extraLabels\":null,\"extraVolumes\":[],\"image\":null,\"nodeSelector\":null,\"ports\":{\"serflan\":{\"port\":8301}},\"priorityClassName\":\"\",\"replicas\":3,\"resources\":{\"limits\":{\"cpu\":\"100m\",\"memory\":\"100Mi\"},\"requests\":{\"cpu\":\"100m\",\"memory\":\"100Mi\"}},\"securityContext\":{\"fsGroup\":1000,\"runAsGroup\":1000,\"runAsNonRoot\":true,\"runAsUser\":100},\"service\":{\"annotations\":null},\"storage\":\"10Gi\",\"storageClass\":null,\"tolerations\":\"\",\"updatePartition\":0},\"syncCatalog\":{\"aclSyncToken\":{\"secretKey\":null,\"secretName\":null},\"addK8SNamespaceSuffix\":true,\"affinity\":null,\"consulNamespaces\":{\"consulDestinationNamespace\":\"default\",\"mirroringK8S\":false,\"mirroringK8SPrefix\":\"\"},\"consulNodeName\":\"k8s-sync\",\"consulPrefix\":null,\"consulWriteInterval\":null,\"default\":true,\"enabled\":true,\"image\":null,\"k8sAllowNamespaces\":[\"*\"],\"k8sDenyNamespaces\":[\"kube-system\",\"kube-public\"],\"k8sPrefix\":null,\"k8sSourceNamespace\":null,\"k8sTag\":null,\"logLevel\":\"info\",\"nodePortSyncType\":\"ExternalFirst\",\"nodeSelector\":null,\"priorityClassName\":\"\",\"resources\":{\"limits\":{\"cpu\":\"50m\",\"memory\":\"50Mi\"},\"requests\":{\"cpu\":\"50m\",\"memory\":\"50Mi\"}},\"syncClusterIPServices\":true,\"toConsul\":true,\"toK8S\":true,\"tolerations\":null},\"terminatingGateways\":{\"defaults\":{\"affinity\":\"podAntiAffinity:\\n  requiredDuringSchedulingIgnoredDuringExecution:\\n    - labelSelector:\\n        matchLabels:\\n          app: {{ template \\\"consul.name\\\" . }}\\n          release: \\\"{{ .Release.Name }}\\\"\\n          component: terminating-gateway\\n      topologyKey: kubernetes.io/hostname\\n\",\"annotations\":null,\"consulNamespace\":\"default\",\"extraVolumes\":[],\"initCopyConsulContainer\":{\"resources\":{\"limits\":{\"cpu\":\"50m\",\"memory\":\"150Mi\"},\"requests\":{\"cpu\":\"50m\",\"memory\":\"25Mi\"}}},\"nodeSelector\":null,\"priorityClassName\":\"\",\"replicas\":2,\"resources\":{\"limits\":{\"cpu\":\"100m\",\"memory\":\"100Mi\"},\"requests\":{\"cpu\":\"100m\",\"memory\":\"100Mi\"}},\"tolerations\":null},\"enabled\":false,\"gateways\":[{\"name\":\"terminating-gateway\"}]},\"tests\":{\"enabled\":true},\"ui\":{\"enabled\":\"-\",\"service\":{\"additionalSpec\":null,\"annotations\":null,\"enabled\":true,\"type\":null}}}",
                "version": "0.28.0"
              }
            ],
            "name": "consul",
            "namespace": "default",
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": null,
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [
              {
                "name": "client.enabled",
                "type": "",
                "value": "true"
              },
              {
                "name": "client.grpc",
                "type": "",
                "value": "true"
              },
              {
                "name": "connectInject.enabled",
                "type": "",
                "value": "true"
              },
              {
                "name": "syncCatalog.enabled",
                "type": "",
                "value": "true"
              }
            ],
            "set_sensitive": [],
            "set_string": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "values": [
              "# Available parameters and their default values for the Consul chart.\n\n# Holds values that affect multiple components of the chart.\nglobal:\n  # The main enabled/disabled setting. If true, servers,\n  # clients, Consul DNS and the Consul UI will be enabled. Each component can override\n  # this default via its component-specific \"enabled\" config. If false, no components\n  # will be installed by default and per-component opt-in is required, such as by\n  # setting `server.enabled` to true.\n  enabled: true\n\n  # Set the prefix used for all resources in the Helm chart. If not set,\n  # the prefix will be `\u003chelm release name\u003e-consul`.\n  # @type: string\n  name: null\n\n  # The domain Consul will answer DNS queries for\n  # (see `-domain` (https://consul.io/docs/agent/options#_domain)) and the domain services synced from\n  # Consul into Kubernetes will have, e.g. `service-name.service.consul`.\n  domain: consul\n\n  # The name (and tag) of the Consul Docker image for clients and servers.\n  # This can be overridden per component. This should be pinned to a specific\n  # version tag, otherwise you may inadvertently upgrade your Consul version.\n  #\n  # Examples:\n  #\n  # ```yaml\n  # # Consul 1.5.0\n  # image: \"consul:1.5.0\"\n  # # Consul Enterprise 1.5.0\n  # image: \"hashicorp/consul-enterprise:1.5.0-ent\"\n  # ```\n  # @default: hashicorp/consul:\u003clatest version\u003e\n  image: \"hashicorp/consul:1.9.1\"\n\n  # Array of objects containing image pull secret names that will be applied to each service account.\n  # This can be used to reference image pull secrets if using a custom consul or consul-k8s Docker image.\n  # See https://kubernetes.io/docs/concepts/containers/images/#using-a-private-registry for reference.\n  #\n  # Example:\n  #\n  # ```yaml\n  # imagePullSecrets:\n  #   - name: pull-secret-name\n  #   - name: pull-secret-name-2\n  # ```\n  # @type: array\u003cmap\u003e\n  imagePullSecrets: []\n\n  # The name (and tag) of the consul-k8s (https://github.com/hashicorp/consul-k8s)\n  # Docker image that is used for functionality such the catalog sync.\n  # This can be overridden per component.\n  # @default: hashicorp/consul-k8s:\u003clatest version\u003e\n  imageK8S: \"hashicorp/consul-k8s:0.22.0\"\n\n  # The name of the datacenter that the agents should\n  # register as. This can't be changed once the Consul cluster is up and running\n  # since Consul doesn't support an automatic way to change this value currently:\n  # https://github.com/hashicorp/consul/issues/1858.\n  datacenter: dc1\n\n  # Controls whether pod security policies are created for the Consul components\n  # created by this chart. See https://kubernetes.io/docs/concepts/policy/pod-security-policy/.\n  enablePodSecurityPolicies: false\n\n  # Configures which Kubernetes secret to retrieve Consul's\n  # gossip encryption key from (see `-encrypt` (https://consul.io/docs/agent/options#_encrypt)). If secretName or\n  # secretKey are not set, gossip encryption will not be enabled. The secret must\n  # be in the same namespace that Consul is installed into.\n  #\n  # The secret can be created by running:\n  #\n  # ```shell\n  # $ kubectl create secret generic consul-gossip-encryption-key --from-literal=key=$(consul keygen)\n  # ```\n  #\n  # To reference, use:\n  #\n  # ```yaml\n  # global:\n  #   gossipEncryption:\n  #     secretName: consul-gossip-encryption-key\n  #     secretKey: key\n  # ```\n  gossipEncryption:\n    # secretName is the name of the Kubernetes secret that holds the gossip\n    # encryption key. The secret must be in the same namespace that Consul is installed into.\n    secretName: \"\"\n    # secretKey is the key within the Kubernetes secret that holds the gossip\n    # encryption key.\n    secretKey: \"\"\n\n  # Enables TLS (https://learn.hashicorp.com/tutorials/consul/tls-encryption-secure)\n  # across the cluster to verify authenticity of the Consul servers and clients.\n  # Requires Consul v1.4.1+ and consul-k8s v0.16.2+\n  tls:\n    # If true, the Helm chart will enable TLS for Consul\n    # servers and clients and all consul-k8s components, as well as generate certificate\n    # authority (optional) and server and client certificates.\n    enabled: false\n\n    # If true, turns on the auto-encrypt feature on clients and servers.\n    # It also switches consul-k8s components to retrieve the CA from the servers\n    # via the API. Requires Consul 1.7.1+ and consul-k8s 0.13.0\n    enableAutoEncrypt: false\n\n    # A list of additional DNS names to set as Subject Alternative Names (SANs)\n    # in the server certificate. This is useful when you need to access the\n    # Consul server(s) externally, for example, if you're using the UI.\n    # @type: array\u003cstring\u003e\n    serverAdditionalDNSSANs: []\n\n    # A list of additional IP addresses to set as Subject Alternative Names (SANs)\n    # in the server certificate. This is useful when you need to access the\n    # Consul server(s) externally, for example, if you're using the UI.\n    # @type: array\u003cstring\u003e\n    serverAdditionalIPSANs: []\n\n    # If true, `verify_outgoing`, `verify_server_hostname`,\n    # and `verify_incoming_rpc` will be set to `true` for Consul servers and clients.\n    # Set this to false to incrementally roll out TLS on an existing Consul cluster.\n    # Please see https://consul.io/docs/k8s/operations/tls-on-existing-cluster\n    # for more details.\n    verify: true\n\n    # If true, the Helm chart will configure Consul to disable the HTTP port on\n    # both clients and servers and to only accept HTTPS connections.\n    httpsOnly: true\n\n    # A Kubernetes secret containing the certificate of the CA to use for\n    # TLS communication within the Consul cluster. If you have generated the CA yourself\n    # with the consul CLI, you could use the following command to create the secret\n    # in Kubernetes:\n    #\n    # ```bash\n    # kubectl create secret generic consul-ca-cert \\\n    #     --from-file='tls.crt=./consul-agent-ca.pem'\n    # ```\n    caCert:\n      # The name of the Kubernetes secret.\n      secretName: null\n      # The key of the Kubernetes secret.\n      secretKey: null\n\n    # A Kubernetes secret containing the private key of the CA to use for\n    # TLS communication within the Consul cluster. If you have generated the CA yourself\n    # with the consul CLI, you could use the following command to create the secret\n    # in Kubernetes:\n    #\n    # ```bash\n    # kubectl create secret generic consul-ca-key \\\n    #     --from-file='tls.key=./consul-agent-ca-key.pem'\n    # ```\n    #\n    # Note that we need the CA key so that we can generate server and client certificates.\n    # It is particularly important for the client certificates since they need to have host IPs\n    # as Subject Alternative Names. In the future, we may support bringing your own server\n    # certificates.\n    caKey:\n      # The name of the Kubernetes secret.\n      secretName: null\n      # The key of the Kubernetes secret.\n      secretKey: null\n\n  # [Enterprise Only] `enableConsulNamespaces` indicates that you are running\n  # Consul Enterprise v1.7+ with a valid Consul Enterprise license and would\n  # like to make use of configuration beyond registering everything into\n  # the `default` Consul namespace. Requires consul-k8s v0.12+. Additional configuration\n  # options are found in the `consulNamespaces` section of both the catalog sync\n  # and connect injector.\n  enableConsulNamespaces: false\n\n  # Configure ACLs.\n  acls:\n\n    # If true, the Helm chart will automatically manage ACL tokens and policies\n    # for all Consul and consul-k8s components.\n    # This requires Consul \u003e= 1.4 and consul-k8s \u003e= 0.14.0.\n    manageSystemACLs: false\n\n    # A Kubernetes secret containing the bootstrap token to use for\n    # creating policies and tokens for all Consul and consul-k8s components.\n    # If set, we will skip ACL bootstrapping of the servers and will only\n    # initialize ACLs for the Consul clients and consul-k8s system components.\n    # Requires consul-k8s \u003e= 0.14.0.\n    bootstrapToken:\n      # The name of the Kubernetes secret.\n      secretName: null\n      # The key of the Kubernetes secret.\n      secretKey: null\n\n    # If true, an ACL token will be created that can be used in secondary\n    # datacenters for replication. This should only be set to true in the\n    # primary datacenter since the replication token must be created from that\n    # datacenter.\n    # In secondary datacenters, the secret needs to be imported from the primary\n    # datacenter and referenced via `global.acls.replicationToken`.\n    # Requires consul-k8s \u003e= 0.13.0.\n    createReplicationToken: false\n\n    # replicationToken references a secret containing the replication ACL token.\n    # This token will be used by secondary datacenters to perform ACL replication\n    # and create ACL tokens and policies.\n    # This value is ignored if `bootstrapToken` is also set.\n    # Requires consul-k8s \u003e= 0.13.0.\n    replicationToken:\n      # The name of the Kubernetes secret.\n      secretName: null\n      # The key of the Kubernetes secret.\n      secretKey: null\n\n  # Configure federation.\n  federation:\n    # If enabled, this datacenter will be federation-capable. Only federation\n    # via mesh gateways is supported.\n    # Mesh gateways and servers will be configured to allow federation.\n    # Requires `global.tls.enabled`, `meshGateway.enabled` and `connectInject.enabled`\n    # to be true. Requires Consul 1.8+.\n    enabled: false\n\n    # If true, the chart will create a Kubernetes secret that can be imported\n    # into secondary datacenters so they can federate with this datacenter. The\n    # secret contains all the information secondary datacenters need to contact\n    # and authenticate with this datacenter. This should only be set to true\n    # in your primary datacenter. The secret name is\n    # `\u003cglobal.name\u003e-federation` (if setting `global.name`), otherwise\n    # `\u003chelm-release-name\u003e-consul-federation`. Requires consul-k8s 0.15.0+.\n    createFederationSecret: false\n\n  # The lifecycle sidecar ensures the Consul services\n  # are always registered with their local Consul clients and is used by the\n  # ingress/terminating/mesh gateways as well as with every Connect-injected service.\n  # @recurse: false\n  # @type: map\n  lifecycleSidecarContainer:\n    resources:\n      requests:\n        memory: \"25Mi\"\n        cpu: \"20m\"\n      limits:\n        memory: \"50Mi\"\n        cpu: \"20m\"\n\n  # The name (and tag) of the Envoy Docker image used for the\n  # connect-injected sidecar proxies and mesh, terminating, and ingress gateways.\n  # See https://www.consul.io/docs/connect/proxies/envoy for full compatibility matrix between Consul and Envoy.\n  # @default: envoyproxy/envoy-alpine:\u003clatest supported version\u003e\n  imageEnvoy: \"envoyproxy/envoy-alpine:v1.16.0\"\n\n  # Configuration for running this Helm chart on the Red Hat OpenShift platform.\n  # This Helm chart currently supports OpenShift v4.x+.\n  openshift:\n    # If true, the Helm chart will create necessary configuration for running\n    # its components on OpenShift.\n    enabled: false\n\n# Server, when enabled, configures a server cluster to run. This should\n# be disabled if you plan on connecting to a Consul cluster external to\n# the Kube cluster.\nserver:\n\n  # If true, the chart will install all the resources necessary for a\n  # Consul server cluster. If you're running Consul externally and want agents\n  # within Kubernetes to join that cluster, this should probably be false.\n  # @default: global.enabled\n  # @type: boolean\n  enabled: \"-\"\n\n  # The name of the Docker image (including any tag) for the containers running\n  # Consul server agents.\n  # @type: string\n  image: null\n\n  # The number of server agents to run. This determines the fault tolerance of\n  # the cluster. Please see the deployment table (https://consul.io/docs/internals/consensus#deployment-table)\n  # for more information.\n  replicas: 3\n\n  # The number of servers that are expected to be running.\n  # It defaults to server.replicas.\n  # In most cases the default should be used, however if there are more\n  # servers in this datacenter than server.replicas it might make sense\n  # to override the default. This would be the case if two kube clusters\n  # were joined into the same datacenter and each cluster ran a certain number\n  # of servers.\n  # @type: int\n  bootstrapExpect: null\n\n  # [Enterprise Only] This value refers to a Kubernetes secret that you have created\n  # that contains your enterprise license. It is required if you are using an\n  # enterprise binary. Defining it here applies it to your cluster once a leader\n  # has been elected. If you are not using an enterprise image or if you plan to\n  # introduce the license key via another route, then set these fields to null.\n  # Note: the job to apply license runs on both Helm installs and upgrades.\n  enterpriseLicense:\n    # The name of the Kubernetes secret that holds the enterprise license.\n    # The secret must be in the same namespace that Consul is installed into.\n    secretName: null\n    # The key within the Kubernetes secret that holds the enterprise license.\n    secretKey: null\n\n  # Exposes the servers' gossip and RPC ports as hostPorts. To enable a client\n  # agent outside of the k8s cluster to join the datacenter, you would need to\n  # enable `server.exposeGossipAndRPCPorts`, `client.exposeGossipPorts`, and\n  # set `server.ports.serflan.port` to a port not being used on the host. Since\n  # `client.exposeGossipPorts` uses the hostPort 8301,\n  # `server.ports.serflan.port` must be set to something other than 8301.\n  exposeGossipAndRPCPorts: false\n\n  # Configures ports for the consul servers.\n  ports:\n    # Configures the LAN gossip port for the consul servers. If you choose to\n    # enable `server.exposeGossipAndRPCPorts` and `client.exposeGossipPorts`,\n    # that will configure the LAN gossip ports on the servers and clients to be\n    # hostPorts, so if you are running clients and servers on the same node the\n    # ports will conflict if they are both 8301.  When you enable\n    # `server.exposeGossipAndRPCPorts` and `client.exposeGossipPorts`, you must\n    # change this from the default to an unused port on the host, e.g. 9301. By\n    # default the LAN gossip port is 8301 and configured as a containerPort on\n    # the consul server Pods.\n    serflan:\n      port: 8301\n\n  # This defines the disk size for configuring the\n  # servers' StatefulSet storage. For dynamically provisioned storage classes, this is the\n  # desired size. For manually defined persistent volumes, this should be set to\n  # the disk size of the attached volume.\n  storage: 10Gi\n\n  # The StorageClass to use for the servers' StatefulSet storage. It must be\n  # able to be dynamically provisioned if you want the storage\n  # to be automatically created. For example, to use local\n  # (https://kubernetes.io/docs/concepts/storage/storage-classes/#local)\n  # storage classes, the PersistentVolumeClaims would need to be manually created.\n  # A `null` value will use the Kubernetes cluster's default StorageClass. If a default\n  # StorageClass does not exist, you will need to create one.\n  # @type: string\n  storageClass: null\n\n  # This will enable/disable Connect (https://consul.io/docs/connect). Setting this to true\n  # _will not_ automatically secure pod communication, this\n  # setting will only enable usage of the feature. Consul will automatically initialize\n  # a new CA and set of certificates. Additional Connect settings can be configured\n  # by setting the `server.extraConfig` value.\n  connect: true\n\n  # The resource requests (CPU, memory, etc.)\n  # for each of the server agents. This should be a YAML map corresponding to a Kubernetes\n  # ResourceRequirements (https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.11/#resourcerequirements-v1-core)\n  # object. NOTE: The use of a YAML string is deprecated.\n  #\n  # Example:\n  #\n  # ```yaml\n  # resources:\n  #   requests:\n  #     memory: '100Mi'\n  #     cpu: '100m'\n  #   limits:\n  #     memory: '100Mi'\n  #     cpu: '100m'\n  # ```\n  #\n  # @recurse: false\n  # @type: map\n  resources:\n    requests:\n      memory: \"100Mi\"\n      cpu: \"100m\"\n    limits:\n      memory: \"100Mi\"\n      cpu: \"100m\"\n\n  # The security context for the server pods. This should be a YAML map corresponding to a\n  # Kubernetes [SecurityContext](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/) object.\n  # By default, servers will run as non-root, with user ID `100` and group ID `1000`,\n  # which correspond to the consul user and group created by the Consul docker image.\n  # Note: if running on OpenShift, this setting is ignored because the user and group are set automatically\n  # by the OpenShift platform.\n  # @type: map\n  # @recurse: false\n  securityContext:\n    runAsNonRoot: true\n    runAsGroup: 1000\n    runAsUser: 100\n    fsGroup: 1000\n\n  # This value is used to carefully\n  # control a rolling update of Consul server agents. This value specifies the\n  # partition (https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#partitions)\n  # for performing a rolling update. Please read the linked Kubernetes documentation\n  # and https://www.consul.io/docs/k8s/upgrade#upgrading-consul-servers for more information.\n  updatePartition: 0\n\n  # This configures the PodDisruptionBudget (https://kubernetes.io/docs/tasks/run-application/configure-pdb/)\n  # for the server cluster.\n  disruptionBudget:\n    # This will enable/disable registering a PodDisruptionBudget for the server\n    # cluster. If this is enabled, it will only register the budget so long as\n    # the server cluster is enabled.\n    enabled: true\n\n    # The maximum number of unavailable pods. By default, this will be\n    # automatically computed based on the `server.replicas` value to be `(n/2)-1`.\n    # If you need to set this to `0`, you will need to add a\n    # --set 'server.disruptionBudget.maxUnavailable=0'` flag to the helm chart installation\n    # command because of a limitation in the Helm templating language.\n    # @type: integer\n    maxUnavailable: null\n\n  # A raw string of extra JSON configuration (https://consul.io/docs/agent/options) for Consul\n  # servers. This will be saved as-is into a ConfigMap that is read by the Consul\n  # server agents. This can be used to add additional configuration that\n  # isn't directly exposed by the chart.\n  #\n  # Example:\n  #\n  # ```yaml\n  # extraConfig: |\n  #   {\n  #     \"log_level\": \"DEBUG\"\n  #   }\n  # ```\n  #\n  # This can also be set using Helm's `--set` flag using the following syntax:\n  #\n  # ```shell\n  # --set 'server.extraConfig=\"{\"log_level\": \"DEBUG\"}\"'\n  # ```\n  extraConfig: |\n    {}\n\n  # A list of extra volumes to mount for server agents. This\n  # is useful for bringing in extra data that can be referenced by other configurations\n  # at a well known path, such as TLS certificates or Gossip encryption keys. The\n  # value of this should be a list of objects.\n  #\n  # Example:\n  #\n  # ```yaml\n  # extraVolumes:\n  #   - type: secret\n  #     name: consul-certs\n  #     load: false\n  # ```\n  #\n  # Each object supports the following keys:\n  #\n  # - `type` - Type of the volume, must be one of \"configMap\" or \"secret\". Case sensitive.\n  #\n  # - `name` - Name of the configMap or secret to be mounted. This also controls\n  #   the path that it is mounted to. The volume will be mounted to `/consul/userconfig/\u003cname\u003e`.\n  #\n  # - `load` - If true, then the agent will be\n  #   configured to automatically load HCL/JSON configuration files from this volume\n  #   with `-config-dir`. This defaults to false.\n  #\n  # @type: array\u003cmap\u003e\n  extraVolumes: []\n\n  # This value defines the affinity (https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity)\n  # for server pods. It defaults to allowing only a single server pod on each node, which\n  # minimizes risk of the cluster becoming unusable if a node is lost. If you need\n  # to run more pods per node (for example, testing on Minikube), set this value\n  # to `null`.\n  #\n  # Example:\n  #\n  # ```yaml\n  # affinity: |\n  #   podAntiAffinity:\n  #     requiredDuringSchedulingIgnoredDuringExecution:\n  #       - labelSelector:\n  #           matchLabels:\n  #             app: {{ template \"consul.name\" . }}\n  #             release: \"{{ .Release.Name }}\"\n  #             component: server\n  #       topologyKey: kubernetes.io/hostname\n  # ```\n  affinity: |\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              app: {{ template \"consul.name\" . }}\n              release: \"{{ .Release.Name }}\"\n              component: server\n          topologyKey: kubernetes.io/hostname\n\n  # Toleration settings for server pods. This\n  # should be a multi-line string matching the Tolerations\n  # (https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/) array in a Pod spec.\n  tolerations: \"\"\n\n  # This value defines `nodeSelector` (https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector)\n  # labels for server pod assignment, formatted as a multi-line string.\n  #\n  # Example:\n  #\n  # ```yaml\n  # nodeSelector: |\n  #   beta.kubernetes.io/arch: amd64\n  # ```\n  #\n  # @type: string\n  nodeSelector: null\n\n  # This value references an existing\n  # Kubernetes `priorityClassName` (https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#pod-priority)\n  # that can be assigned to server pods.\n  priorityClassName: \"\"\n\n  # Extra labels to attach to the server pods. This should be a YAML map.\n  #\n  # Example:\n  #\n  # ```yaml\n  # extraLabels:\n  #   labelKey: label-value\n  #   anotherLabelKey: another-label-value\n  # ```\n  #\n  # @type: map\n  extraLabels: null\n\n  # This value defines additional annotations for\n  # server pods. This should be formatted as a multi-line string.\n  #\n  # ```yaml\n  # annotations: |\n  #   \"sample/annotation1\": \"foo\"\n  #   \"sample/annotation2\": \"bar\"\n  # ```\n  #\n  # @type: string\n  annotations: null\n\n  # Server service properties.\n  service:\n    # Annotations to apply to the server service.\n    #\n    # ```yaml\n    # annotations: |\n    #   \"annotation-key\": \"annotation-value\"\n    # ```\n    #\n    # @type: string\n    annotations: null\n\n  # A list of extra environment variables to set within the stateful set.\n  # These could be used to include proxy settings required for cloud auto-join\n  # feature, in case kubernetes cluster is behind egress http proxies. Additionally,\n  # it could be used to configure custom consul parameters.\n  # @type: map\n  extraEnvironmentVars: {}\n\n# Configuration for Consul servers when the servers are running outside of Kubernetes.\n# When running external servers, configuring these values is recommended\n# if setting `global.tls.enableAutoEncrypt` to true (requires consul-k8s \u003e= 0.13.0)\n# or `global.acls.manageSystemACLs` to true (requires consul-k8s \u003e= 0.14.0).\nexternalServers:\n  # If true, the Helm chart will be configured to talk to the external servers.\n  # If setting this to true, you must also set `server.enabled` to false.\n  enabled: false\n\n  # An array of external Consul server hosts that are used to make\n  # HTTPS connections from the components in this Helm chart.\n  # Valid values include IPs, DNS names, or Cloud auto-join string.\n  # The port must be provided separately below.\n  # Note: `client.join` must also be set to the hosts that should be\n  # used to join the cluster. In most cases, the `client.join` values\n  # should be the same, however, they may be different if you\n  # wish to use separate hosts for the HTTPS connections.\n  # @type: array\u003cstring\u003e\n  hosts: []\n\n  # The HTTPS port of the Consul servers.\n  httpsPort: 8501\n\n  # The server name to use as the SNI host header when connecting with HTTPS.\n  # @type: string\n  tlsServerName: null\n\n  # If true, consul-k8s components will ignore the CA set in\n  # `global.tls.caCert` when making HTTPS calls to Consul servers and\n  # will instead use the consul-k8s image's system CAs for TLS verification.\n  # If false, consul-k8s components will use `global.tls.caCert` when\n  # making HTTPS calls to Consul servers.\n  # **NOTE:** This does not affect Consul's internal RPC communication which will\n  # always use `global.tls.caCert`.\n  useSystemRoots: false\n\n  # If you are setting `global.acls.manageSystemACLs` and\n  # `connectInject.enabled` to true, set `k8sAuthMethodHost` to the address of the Kubernetes API server.\n  # This address must be reachable from the Consul servers.\n  # Please see the Kubernetes Auth Method documentation (https://consul.io/docs/acl/auth-methods/kubernetes).\n  # Requires consul-k8s \u003e= 0.14.0.\n  #\n  # You could retrieve this value from your `kubeconfig` by running:\n  #\n  # ```shell\n  # kubectl config view \\\n  #   -o jsonpath=\"{.clusters[?(@.name=='\u003cyour cluster name\u003e')].cluster.server}\"\n  # ```\n  #\n  # @type: string\n  k8sAuthMethodHost: null\n\n# Values that configure running a Consul client on Kubernetes nodes.\nclient:\n  # If true, the chart will install all\n  # the resources necessary for a Consul client on every Kubernetes node. This _does not_ require\n  # `server.enabled`, since the agents can be configured to join an external cluster.\n  # @default: global.enabled\n  # @type: boolean\n  enabled: \"-\"\n\n  # The name of the Docker image (including any tag) for the containers\n  # running Consul client agents.\n  # @type: string\n  image: null\n\n  # A list of valid `-retry-join` values (https://consul.io/docs/agent/options#retry-join).\n  # If this is `null` (default), then the clients will attempt to automatically\n  # join the server cluster running within Kubernetes.\n  # This means that with `server.enabled` set to true, clients will automatically\n  # join that cluster. If `server.enabled` is not true, then a value must be\n  # specified so the clients can join a valid cluster.\n  # @type: array\u003cstring\u003e\n  join: null\n\n  # An absolute path to a directory on the host machine to use as the Consul\n  # client data directory. If set to the empty string or null, the Consul agent\n  # will store its data in the Pod's local filesystem (which will\n  # be lost if the Pod is deleted). Security Warning: If setting this, Pod Security\n  # Policies _must_ be enabled on your cluster and in this Helm chart (via the\n  # `global.enablePodSecurityPolicies` setting) to prevent other pods from\n  # mounting the same host path and gaining access to all of Consul's data.\n  # Consul's data is not encrypted at rest.\n  # @type: string\n  dataDirectoryHostPath: null\n\n  # If true, agents will enable their GRPC listener on\n  # port 8502 and expose it to the host. This will use slightly more resources, but is\n  # required for Connect.\n  grpc: true\n\n  # If true, the Helm chart will expose the clients' gossip ports as hostPorts.\n  # This is only necessary if pod IPs in the k8s cluster are not directly routable\n  # and the Consul servers are outside of the k8s cluster.\n  # This also changes the clients' advertised IP to the `hostIP` rather than `podIP`.\n  exposeGossipPorts: false\n\n  # Resource settings for Client agents.\n  # NOTE: The use of a YAML string is deprecated. Instead, set directly as a\n  # YAML map.\n  # @recurse: false\n  # @type: map\n  resources:\n    requests:\n      memory: \"100Mi\"\n      cpu: \"100m\"\n    limits:\n      memory: \"100Mi\"\n      cpu: \"100m\"\n\n  # The security context for the client pods. This should be a YAML map corresponding to a\n  # Kubernetes [SecurityContext](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/) object.\n  # By default, servers will run as non-root, with user ID `100` and group ID `1000`,\n  # which correspond to the consul user and group created by the Consul docker image.\n  # Note: if running on OpenShift, this setting is ignored because the user and group are set automatically\n  # by the OpenShift platform.\n  # @type: map\n  # @recurse: false\n  securityContext:\n    runAsNonRoot: true\n    runAsGroup: 1000\n    runAsUser: 100\n    fsGroup: 1000\n\n  # A raw string of extra JSON configuration (https://consul.io/docs/agent/options) for Consul\n  # clients. This will be saved as-is into a ConfigMap that is read by the Consul\n  # client agents. This can be used to add additional configuration that\n  # isn't directly exposed by the chart.\n  #\n  # Example:\n  #\n  # ```yaml\n  # extraConfig: |\n  #   {\n  #     \"log_level\": \"DEBUG\"\n  #   }\n  # ```\n  #\n  # This can also be set using Helm's `--set` flag using the following syntax:\n  #\n  # ```shell\n  # --set 'client.extraConfig=\"{\"log_level\": \"DEBUG\"}\"'\n  # ```\n  extraConfig: |\n    {}\n\n  # A list of extra volumes to mount for client agents. This\n  # is useful for bringing in extra data that can be referenced by other configurations\n  # at a well known path, such as TLS certificates or Gossip encryption keys. The\n  # value of this should be a list of objects.\n  #\n  # Example:\n  #\n  # ```yaml\n  # extraVolumes:\n  #   - type: secret\n  #     name: consul-certs\n  #     load: false\n  # ```\n  #\n  # Each object supports the following keys:\n  #\n  # - `type` - Type of the volume, must be one of \"configMap\" or \"secret\". Case sensitive.\n  #\n  # - `name` - Name of the configMap or secret to be mounted. This also controls\n  #   the path that it is mounted to. The volume will be mounted to `/consul/userconfig/\u003cname\u003e`.\n  #\n  # - `load` - If true, then the agent will be\n  #   configured to automatically load HCL/JSON configuration files from this volume\n  #   with `-config-dir`. This defaults to false.\n  #\n  # @type: array\u003cmap\u003e\n  extraVolumes: []\n\n  # Toleration Settings for Client pods\n  # This should be a multi-line string matching the Toleration array\n  # in a PodSpec.\n  # The example below will allow Client pods to run on every node\n  # regardless of taints\n  #\n  # ```yaml\n  # tolerations: |\n  #   - operator: Exists\n  # ```\n  tolerations: \"\"\n\n  # nodeSelector labels for client pod assignment, formatted as a multi-line string.\n  # ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector\n  #\n  # Example:\n  #\n  # ```yaml\n  # nodeSelector: |\n  #   beta.kubernetes.io/arch: amd64\n  # ```\n  # @type: string\n  nodeSelector: null\n\n  # Affinity Settings for Client pods, formatted as a multi-line YAML string.\n  # ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\n  #\n  # Example:\n  #\n  # ```yaml\n  # affinity: |\n  #   nodeAffinity:\n  #     requiredDuringSchedulingIgnoredDuringExecution:\n  #       nodeSelectorTerms:\n  #       - matchExpressions:\n  #         - key: node-role.kubernetes.io/master\n  #           operator: DoesNotExist\n  # ```\n  # @type: string\n  affinity: {}\n\n  # This value references an existing\n  # Kubernetes `priorityClassName` (https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#pod-priority)\n  # that can be assigned to client pods.\n  priorityClassName: \"\"\n\n  # This value defines additional annotations for\n  # client pods. This should be formatted as a multi-line string.\n  #\n  # ```yaml\n  # annotations: |\n  #   \"sample/annotation1\": \"foo\"\n  #   \"sample/annotation2\": \"bar\"\n  # ```\n  #\n  # @type: string\n  annotations: null\n\n  # A list of extra environment variables to set within the stateful set.\n  # These could be used to include proxy settings required for cloud auto-join\n  # feature, in case kubernetes cluster is behind egress http proxies. Additionally,\n  # it could be used to configure custom consul parameters.\n  # @type: map\n  extraEnvironmentVars: {}\n\n  # This value defines the Pod DNS policy (https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-dns-policy)\n  # for client pods to use.\n  # @type: string\n  dnsPolicy: null\n\n  # hostNetwork defines whether or not we use host networking instead of hostPort in the event\n  # that a CNI plugin doesn't support `hostPort`. This has security implications and is not recommended\n  # as doing so gives the consul client unnecessary access to all network traffic on the host.\n  # In most cases, pod network and host network are on different networks so this should be\n  # combined with `dnsPolicy: ClusterFirstWithHostNet`\n  hostNetwork: false\n\n  # updateStrategy for the DaemonSet.\n  # See https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/#daemonset-update-strategy.\n  # This should be a multi-line string mapping directly to the updateStrategy\n  #\n  # Example:\n  #\n  # ```yaml\n  # updateStrategy: |\n  #   rollingUpdate:\n  #     maxUnavailable: 5\n  #   type: RollingUpdate\n  # ```\n  #\n  # @type: string\n  updateStrategy: null\n\n  # [Enterprise Only] Values for setting up and running snapshot agents\n  # (https://consul.io/commands/snapshot/agent)\n  # within the Consul clusters. They are required to be co-located with Consul clients,\n  # so will inherit the clients' nodeSelector, tolerations and affinity.\n  snapshotAgent:\n    # If true, the chart will install resources necessary to run the snapshot agent.\n    enabled: false\n\n    # The number of snapshot agents to run.\n    replicas: 2\n\n    # A Kubernetes secret that should be manually created to contain the entire\n    # config to be used on the snapshot agent.\n    # This is the preferred method of configuration since there are usually storage\n    # credentials present. Please see Snapshot agent config (https://consul.io/commands/snapshot/agent#config-file-options)\n    # for details.\n    configSecret:\n      # The name of the Kubernetes secret.\n      secretName: null\n      # The key of the Kubernetes secret.\n      secretKey: null\n\n    # Resource settings for snapshot agent pods.\n    # @recurse: false\n    # @type: map\n    resources:\n      requests:\n        memory: \"50Mi\"\n        cpu: \"50m\"\n      limits:\n        memory: \"50Mi\"\n        cpu: \"50m\"\n\n    # Optional PEM-encoded CA certificate that will be added to the trusted system CAs.\n    # Useful if using an S3-compatible storage exposing a self-signed certificate.\n    #\n    # Example:\n    #\n    # ```yaml\n    # caCert: |\n    #   -----BEGIN CERTIFICATE-----\n    #   MIIC7jCCApSgAwIBAgIRAIq2zQEVexqxvtxP6J0bXAwwCgYIKoZIzj0EAwIwgbkx\n    #   ...\n    # ```\n    # @type: string\n    caCert: null\n\n# Configuration for DNS configuration within the Kubernetes cluster.\n# This creates a service that routes to all agents (client or server)\n# for serving DNS requests. This DOES NOT automatically configure kube-dns\n# today, so you must still manually configure a `stubDomain` with kube-dns\n# for this to have any effect:\n# https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/#configure-stub-domain-and-upstream-dns-servers\ndns:\n  # @type: boolean\n  enabled: \"-\"\n\n  # Used to control the type of service created. For\n  # example, setting this to \"LoadBalancer\" will create an external load\n  # balancer (for supported K8S installations)\n  type: ClusterIP\n\n  # Set a predefined cluster IP for the DNS service.\n  # Useful if you need to reference the DNS service's IP\n  # address in CoreDNS config.\n  # @type: string\n  clusterIP: null\n\n  # Extra annotations to attach to the dns service\n  # This should be a multi-line string of\n  # annotations to apply to the dns Service\n  # @type: string\n  annotations: null\n\n  # Additional ServiceSpec values\n  # This should be a multi-line string mapping directly to a Kubernetes\n  # ServiceSpec object.\n  # @type: string\n  additionalSpec: null\n\n# Values that configure the Consul UI.\nui:\n  # If true, the UI will be enabled. This will\n  # only _enable_ the UI, it doesn't automatically register any service for external\n  # access. The UI will only be enabled on server agents. If `server.enabled` is\n  # false, then this setting has no effect. To expose the UI in some way, you must\n  # configure `ui.service`.\n  # @default: global.enabled\n  # @type: boolean\n  enabled: \"-\"\n\n  # Configure the service for the Consul UI.\n  service:\n    # This will enable/disable registering a\n    # Kubernetes Service for the Consul UI. This value only takes effect if `ui.enabled` is\n    # true and taking effect.\n    enabled: true\n\n    # The service type to register.\n    # @type: string\n    type: null\n\n    # Annotations to apply to the UI service.\n    #\n    # Example:\n    #\n    # ```yaml\n    # annotations: |\n    #   'annotation-key': annotation-value\n    # ```\n    # @type: string\n    annotations: null\n\n    # Additional ServiceSpec values\n    # This should be a multi-line string mapping directly to a Kubernetes\n    # ServiceSpec object.\n    # @type: string\n    additionalSpec: null\n\n# Configure the catalog sync process to sync K8S with Consul\n# services. This can run bidirectional (default) or unidirectionally (Consul\n# to K8S or K8S to Consul only).\n#\n# This process assumes that a Consul agent is available on the host IP.\n# This is done automatically if clients are enabled. If clients are not\n# enabled then set the node selection so that it chooses a node with a\n# Consul agent.\nsyncCatalog:\n  # True if you want to enable the catalog sync. Set to \"-\" to inherit from\n  # global.enabled.\n  enabled: false\n\n  # The name of the Docker image (including any tag) for consul-k8s\n  # to run the sync program.\n  # @type: string\n  image: null\n\n  # If true, all valid services in K8S are\n  # synced by default. If false, the service must be annotated\n  # (https://consul.io/docs/k8s/service-sync#sync-enable-disable) properly to sync.\n  # In either case an annotation can override the default.\n  default: true\n\n  # Optional priorityClassName.\n  priorityClassName: \"\"\n\n  # If true, will sync Kubernetes services to Consul. This can be disabled to\n  # have a one-way sync.\n  toConsul: true\n\n  # If true, will sync Consul services to Kubernetes. This can be disabled to\n  # have a one-way sync.\n  toK8S: true\n\n  # Service prefix to prepend to services before registering\n  # with Kubernetes. For example \"consul-\" will register all services\n  # prepended with \"consul-\". (Consul -\u003e Kubernetes sync)\n  # @type: string\n  k8sPrefix: null\n\n  # List of k8s namespaces to sync the k8s services from.\n  # If a k8s namespace is not included in this list or is listed in `k8sDenyNamespaces`,\n  # services in that k8s namespace will not be synced even if they are explicitly\n  # annotated. Use `[\"*\"]` to automatically allow all k8s namespaces.\n  #\n  # For example, `[\"namespace1\", \"namespace2\"]` will only allow services in the k8s\n  # namespaces `namespace1` and `namespace2` to be synced and registered\n  # with Consul. All other k8s namespaces will be ignored.\n  #\n  # To deny all namespaces, set this to `[]`.\n  #\n  # Note: `k8sDenyNamespaces` takes precedence over values defined here.\n  # Requires consul-k8s v0.12+\n  # @type: array\u003cstring\u003e\n  k8sAllowNamespaces: [\"*\"]\n\n  # List of k8s namespaces that should not have their\n  # services synced. This list takes precedence over `k8sAllowNamespaces`.\n  # `*` is not supported because then nothing would be allowed to sync.\n  # Requires consul-k8s v0.12+.\n  #\n  # For example, if `k8sAllowNamespaces` is `[\"*\"]` and `k8sDenyNamespaces` is\n  # `[\"namespace1\", \"namespace2\"]`, then all k8s namespaces besides `namespace1`\n  # and `namespace2` will be synced.\n  # @type: array\u003cstring\u003e\n  k8sDenyNamespaces: [\"kube-system\", \"kube-public\"]\n\n  # [DEPRECATED] Use k8sAllowNamespaces and k8sDenyNamespaces instead. For\n  # backwards compatibility, if both this and the allow/deny lists are set,\n  # the allow/deny lists will be ignored.\n  # k8sSourceNamespace is the Kubernetes namespace to watch for service\n  # changes and sync to Consul. If this is not set then it will default\n  # to all namespaces.\n  # @type: string\n  k8sSourceNamespace: null\n\n  # [Enterprise Only] These settings manage the catalog sync's interaction with\n  # Consul namespaces (requires consul-ent v1.7+ and consul-k8s v0.12+).\n  # Also, `global.enableConsulNamespaces` must be true.\n  consulNamespaces:\n    # Name of the Consul namespace to register all\n    # k8s services into. If the Consul namespace does not already exist,\n    # it will be created. This will be ignored if `mirroringK8S` is true.\n    consulDestinationNamespace: \"default\"\n\n    # If true, k8s services will be registered into a Consul namespace\n    # of the same name as their k8s namespace, optionally prefixed if\n    # `mirroringK8SPrefix` is set below. If the Consul namespace does not\n    # already exist, it will be created. Turning this on overrides the\n    # `consulDestinationNamespace` setting.\n    # `addK8SNamespaceSuffix` may no longer be needed if enabling this option.\n    mirroringK8S: false\n\n    # If `mirroringK8S` is set to true, `mirroringK8SPrefix` allows each Consul namespace\n    # to be given a prefix. For example, if `mirroringK8SPrefix` is set to \"k8s-\", a\n    # service in the k8s `staging` namespace will be registered into the\n    # `k8s-staging` Consul namespace.\n    mirroringK8SPrefix: \"\"\n\n  # Appends Kubernetes namespace suffix to\n  # each service name synced to Consul, separated by a dash.\n  # For example, for a service 'foo' in the default namespace,\n  # the sync process will create a Consul service named 'foo-default'.\n  # Set this flag to true to avoid registering services with the same name\n  # but in different namespaces as instances for the same Consul service.\n  # Namespace suffix is not added if 'annotationServiceName' is provided.\n  addK8SNamespaceSuffix: true\n\n  # Service prefix which prepends itself\n  # to Kubernetes services registered within Consul\n  # For example, \"k8s-\" will register all services prepended with \"k8s-\".\n  # (Kubernetes -\u003e Consul sync)\n  # consulPrefix is ignored when 'annotationServiceName' is provided.\n  # NOTE: Updating this property to a non-null value for an existing installation will result in deregistering\n  # of existing services in Consul and registering them with a new name.\n  # @type: string\n  consulPrefix: null\n\n  # Optional tag that is applied to all of the Kubernetes services\n  # that are synced into Consul. If nothing is set, defaults to \"k8s\".\n  # (Kubernetes -\u003e Consul sync)\n  # @type: string\n  k8sTag: null\n\n  # Defines the Consul synthetic node that all services\n  # will be registered to.\n  # NOTE: Changing the node name and upgrading the Helm chart will leave\n  # all of the previously sync'd services registered with Consul and\n  # register them again under the new Consul node name. The out-of-date\n  # registrations will need to be explicitly removed.\n  consulNodeName: \"k8s-sync\"\n\n  # Syncs services of the ClusterIP type, which may\n  # or may not be broadly accessible depending on your Kubernetes cluster.\n  # Set this to false to skip syncing ClusterIP services.\n  syncClusterIPServices: true\n\n  # Configures the type of syncing that happens for NodePort\n  # services. The valid options are: ExternalOnly, InternalOnly, ExternalFirst.\n  #\n  # - ExternalOnly will only use a node's ExternalIP address for the sync\n  # - InternalOnly use's the node's InternalIP address\n  # - ExternalFirst will preferentially use the node's ExternalIP address, but\n  #   if it doesn't exist, it will use the node's InternalIP address instead.\n  nodePortSyncType: ExternalFirst\n\n  # Refers to a Kubernetes secret that you have created that contains\n  # an ACL token for your Consul cluster which allows the sync process the correct\n  # permissions. This is only needed if ACLs are enabled on the Consul cluster.\n  aclSyncToken:\n    # The name of the Kubernetes secret.\n    secretName: null\n    # The key of the Kubernetes secret.\n    secretKey: null\n\n  # This value defines `nodeSelector` (https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector)\n  # labels for catalog sync pod assignment, formatted as a multi-line string.\n  #\n  # Example:\n  #\n  # ```yaml\n  # nodeSelector: |\n  #   beta.kubernetes.io/arch: amd64\n  # ```\n  #\n  # @type: string\n  nodeSelector: null\n\n  # Affinity Settings\n  # This should be a multi-line string matching the affinity object\n  # @type: string\n  affinity: null\n\n  # Toleration Settings\n  # This should be a multi-line string matching the Toleration array\n  # in a PodSpec.\n  # @type: string\n  tolerations: null\n\n  # Resource settings for sync catalog pods.\n  # @recurse: false\n  # @type: map\n  resources:\n    requests:\n      memory: \"50Mi\"\n      cpu: \"50m\"\n    limits:\n      memory: \"50Mi\"\n      cpu: \"50m\"\n\n  # Log verbosity level. One of \"trace\", \"debug\", \"info\", \"warn\", or \"error\".\n  logLevel: info\n\n  # Override the default interval to perform syncing operations creating Consul services.\n  # @type: string\n  consulWriteInterval: null\n\n# Configures the automatic Connect sidecar injector.\nconnectInject:\n  # True if you want to enable connect injection. Set to \"-\" to inherit from\n  # global.enabled.\n  enabled: false\n\n  # Image for consul-k8s that contains the injector\n  # @type: string\n  image: null\n\n  # If true, the injector will inject the\n  # Connect sidecar into all pods by default. Otherwise, pods must specify the\n  # injection annotation (https://consul.io/docs/k8s/connect#consul-hashicorp-com-connect-inject)\n  # to opt-in to Connect injection. If this is true, pods can use the same annotation\n  # to explicitly opt-out of injection.\n  default: false\n\n  # Enables synchronization of Kubernetes health probe status with Consul.\n  # NOTE: It is highly recommended to enable TLS with this feature because it requires\n  # making calls to Consul clients across the cluster. Without TLS enabled, these calls\n  # could leak ACL tokens should the cluster network become compromised.\n  healthChecks:\n    # Enables the Consul Health Check controller which syncs the readiness status of\n    # connect-injected pods with Consul.\n    enabled: true\n    # If `healthChecks.enabled` is set to `true`, `reconcilePeriod` defines how often a full state\n    # reconcile is done after the initial reconcile at startup is completed.\n    reconcilePeriod: \"1m\"\n\n  # Used to pass arguments to the injected envoy sidecar.\n  # Valid arguments to pass to envoy can be found here: https://www.envoyproxy.io/docs/envoy/latest/operations/cli\n  # e.g \"--log-level debug --disable-hot-restart\"\n  # @type: string\n  envoyExtraArgs: null\n\n  # Optional priorityClassName.\n  priorityClassName: \"\"\n\n  # The Docker image for Consul to use when performing Connect injection.\n  # Defaults to global.image.\n  # @type: string\n  imageConsul: null\n\n  # Log verbosity level. One of \"debug\", \"info\", \"warn\", or \"error\".\n  logLevel: info\n\n  # Resource settings for connect inject pods.\n  # @recurse: false\n  # @type: map\n  resources:\n    requests:\n      memory: \"50Mi\"\n      cpu: \"50m\"\n    limits:\n      memory: \"50Mi\"\n      cpu: \"50m\"\n\n  # Selector for restricting the webhook to only\n  # specific namespaces. This should be set to a multiline string.\n  # See https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#matching-requests-namespaceselector\n  # for more details.\n  #\n  # Example:\n  #\n  # ```yaml\n  # namespaceSelector: |\n  #   matchLabels:\n  #     namespace-label: label-value\n  # ```\n  # @type: string\n  namespaceSelector: null\n\n  # List of k8s namespaces to allow Connect sidecar\n  # injection in. If a k8s namespace is not included or is listed in `k8sDenyNamespaces`,\n  # pods in that k8s namespace will not be injected even if they are explicitly\n  # annotated. Use `[\"*\"]` to automatically allow all k8s namespaces.\n  #\n  # For example, `[\"namespace1\", \"namespace2\"]` will only allow pods in the k8s\n  # namespaces `namespace1` and `namespace2` to have Connect sidecars injected\n  # and registered with Consul. All other k8s namespaces will be ignored.\n  #\n  # To deny all namespaces, set this to `[]`.\n  #\n  # Note: `k8sDenyNamespaces` takes precedence over values defined here and\n  # `namespaceSelector` takes precedence over both since it is applied first.\n  # `kube-system` and `kube-public` are never injected, even if included here.\n  # Requires consul-k8s v0.12+\n  # @type: array\u003cstring\u003e\n  k8sAllowNamespaces: [\"*\"]\n\n  # List of k8s namespaces that should not allow Connect\n  # sidecar injection. This list takes precedence over `k8sAllowNamespaces`.\n  # `*` is not supported because then nothing would be allowed to be injected.\n  #\n  # For example, if `k8sAllowNamespaces` is `[\"*\"]` and k8sDenyNamespaces is\n  # `[\"namespace1\", \"namespace2\"]`, then all k8s namespaces besides \"namespace1\"\n  # and \"namespace2\" will be available for injection.\n  #\n  # Note: `namespaceSelector` takes precedence over this since it is applied first.\n  # `kube-system` and `kube-public` are never injected.\n  # Requires consul-k8s v0.12+.\n  # @type: array\u003cstring\u003e\n  k8sDenyNamespaces: []\n\n  # [Enterprise Only] These settings manage the connect injector's interaction with\n  # Consul namespaces (requires consul-ent v1.7+ and consul-k8s v0.12+).\n  # Also, `global.enableConsulNamespaces` must be true.\n  consulNamespaces:\n    # Name of the Consul namespace to register all\n    # k8s pods into. If the Consul namespace does not already exist,\n    # it will be created. This will be ignored if `mirroringK8S` is true.\n    consulDestinationNamespace: \"default\"\n\n    # Causes k8s pods to be registered into a Consul namespace\n    # of the same name as their k8s namespace, optionally prefixed if\n    # `mirroringK8SPrefix` is set below. If the Consul namespace does not\n    # already exist, it will be created. Turning this on overrides the\n    # `consulDestinationNamespace` setting.\n    mirroringK8S: false\n\n    # If `mirroringK8S` is set to true, `mirroringK8SPrefix` allows each Consul namespace\n    # to be given a prefix. For example, if `mirroringK8SPrefix` is set to \"k8s-\", a\n    # pod in the k8s `staging` namespace will be registered into the\n    # `k8s-staging` Consul namespace.\n    mirroringK8SPrefix: \"\"\n\n  # The certs section configures how the webhook TLS certs are configured.\n  # These are the TLS certs for the Kube apiserver communicating to the\n  # webhook. By default, the injector will generate and manage its own certs,\n  # but this requires the ability for the injector to update its own\n  # MutatingWebhookConfiguration. In a production environment, custom certs\n  # should probably be used. Configure the values below to enable this.\n  certs:\n    # Name of the secret that has the TLS certificate and\n    # private key to serve the injector webhook. If this is null, then the\n    # injector will default to its automatic management mode that will assign\n    # a service account to the injector to generate its own certificates.\n    secretName: null\n\n    # Base64-encoded PEM-encoded certificate bundle for the\n    # CA that signed the TLS certificate that the webhook serves. This must\n    # be set if secretName is non-null.\n    caBundle: \"\"\n\n    # Name of the file within the secret for\n    # the TLS cert.\n    certName: tls.crt\n\n    # Name of the file within the secret for\n    # the private TLS key.\n    keyName: tls.key\n\n  # Selector labels for connectInject pod assignment, formatted as a multi-line string.\n  # ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector\n  #\n  # Example:\n  #\n  # ```yaml\n  # nodeSelector: |\n  #   beta.kubernetes.io/arch: amd64\n  # ```\n  # @type: string\n  nodeSelector: null\n\n  # Affinity Settings\n  # This should be a multi-line string matching the affinity object\n  # @type: string\n  affinity: null\n\n  # Toleration Settings\n  # This should be a multi-line string matching the Toleration array\n  # in a PodSpec.\n  # @type: string\n  tolerations: null\n\n  # Query that defines which Service Accounts\n  # can authenticate to Consul and receive an ACL token during Connect injection.\n  # The default setting, i.e. serviceaccount.name!=default, prevents the\n  # 'default' Service Account from logging in.\n  # If set to an empty string all service accounts can log in.\n  # This only has effect if ACLs are enabled.\n  #\n  # See https://www.consul.io/docs/acl/acl-auth-methods.html#binding-rules\n  # and https://www.consul.io/docs/acl/auth-methods/kubernetes.html#trusted-identity-attributes\n  # for more details.\n  # Requires Consul \u003e= v1.5 and consul-k8s \u003e= v0.8.0.\n  aclBindingRuleSelector: \"serviceaccount.name!=default\"\n\n  # If you are not using global.acls.manageSystemACLs and instead manually setting up an\n  # auth method for Connect inject, set this to the name of your auth method.\n  overrideAuthMethodName: \"\"\n\n  # Refers to a Kubernetes secret that you have created that contains\n  # an ACL token for your Consul cluster which allows the Connect injector the correct\n  # permissions. This is only needed if Consul namespaces [Enterprise Only] and ACLs\n  # are enabled on the Consul cluster and you are not setting\n  # `global.acls.manageSystemACLs` to `true`.\n  # This token needs to have `operator = \"write\"` privileges to be able to\n  # create Consul namespaces.\n  aclInjectToken:\n    # The name of the Kubernetes secret.\n    # @type: string\n    secretName: null\n    # The key of the Kubernetes secret.\n    # @type: string\n    secretKey: null\n\n  # Requires Consul \u003e= v1.5 and consul-k8s \u003e= v0.8.1.\n  centralConfig:\n    # Controls whether central config is enabled on all servers and clients.\n    # See https://www.consul.io/docs/agent/options.html#enable_central_service_config.\n    # If changing this after installation, servers and clients must be restarted\n    # for the change to take effect.\n    enabled: true\n\n    # Allows you to specify a convenience default protocol if\n    # most of your services are of the same protocol type. The individual annotation\n    # on any given pod will override this value.\n    # Valid values are \"http\", \"http2\", \"grpc\" and \"tcp\".\n    # @type: string\n    defaultProtocol: null\n\n    # Raw json string that will be written as the value of\n    # the \"config\" key of the global proxy-defaults config entry.\n    # See: https://www.consul.io/docs/agent/config-entries/proxy-defaults.html\n    # NOTE: Changes to this value after the chart is first installed have _no_\n    # effect. In order to change the proxy-defaults config after installation,\n    # you must use the Consul API.\n    proxyDefaults: |\n      {}\n\n  sidecarProxy:\n    # Set default resources for sidecar proxy. If null, that resource won't\n    # be set.\n    # These settings can be overridden on a per-pod basis via these annotations:\n    #\n    # - `consul.hashicorp.com/sidecar-proxy-cpu-limit`\n    # - `consul.hashicorp.com/sidecar-proxy-cpu-request`\n    # - `consul.hashicorp.com/sidecar-proxy-memory-limit`\n    # - `consul.hashicorp.com/sidecar-proxy-memory-request`\n    # @type: map\n    resources:\n      requests:\n        # Recommended default: 100Mi\n        # @type: string\n        memory: null\n        # Recommended default: 100m\n        # @type: string\n        cpu: null\n      limits:\n        # Recommended default: 100Mi\n        # @type: string\n        memory: null\n        # Recommended default: 100m\n        # @type: string\n        cpu: null\n\n  # Resource settings for the Connect injected init container.\n  # @recurse: false\n  # @type: map\n  initContainer:\n    resources:\n      requests:\n        memory: \"25Mi\"\n        cpu: \"50m\"\n      limits:\n        memory: \"150Mi\"\n        cpu: \"50m\"\n\n# Controller handles config entry custom resources.\n# Requires consul \u003e= 1.8.4.\n# ServiceIntentions require consul 1.9+.\ncontroller:\n  # Enables the controller for managing custom resources.\n  enabled: false\n\n  # The number of deployment replicas.\n  replicas: 1\n\n  # Log verbosity level. One of \"debug\", \"info\", \"warn\", or \"error\".\n  logLevel: info\n\n  # Resource settings for controller pods.\n  # @recurse: false\n  # @type: map\n  resources:\n    limits:\n      cpu: 100m\n      memory: 50Mi\n    requests:\n      cpu: 100m\n      memory: 50Mi\n\n  # Optional YAML string to specify a nodeSelector config.\n  # @type: string\n  nodeSelector: null\n\n  # Optional YAML string to specify tolerations.\n  # @type: string\n  tolerations: null\n\n  # Affinity Settings\n  # This should be a multi-line string matching the affinity object\n  # @type: string\n  affinity: null\n\n  # Optional priorityClassName.\n  priorityClassName: \"\"\n\n# Mesh Gateways enable Consul Connect to work across Consul datacenters.\nmeshGateway:\n  # If mesh gateways are enabled, a Deployment will be created that runs\n  # gateways and Consul Connect will be configured to use gateways.\n  # See https://www.consul.io/docs/connect/mesh_gateway.html\n  # Requirements: consul 1.6.0+ and consul-k8s 0.15.0+ if using\n  # global.acls.manageSystemACLs.\n  enabled: false\n\n  # Globally configure which mode the gateway should run in.\n  # Can be set to either \"remote\", \"local\", \"none\" or empty string or null.\n  # See https://consul.io/docs/connect/mesh_gateway.html#modes-of-operation for\n  # a description of each mode.\n  # If set to anything other than \"\" or null, connectInject.centralConfig.enabled\n  # should be set to true so that the global config will actually be used.\n  # If set to the empty string, no global default will be set and the gateway mode\n  # will need to be set individually for each service.\n  globalMode: local\n\n  # Number of replicas for the Deployment.\n  replicas: 2\n\n  # What gets registered as WAN address for the gateway.\n  wanAddress:\n    # source configures where to retrieve the WAN address (and possibly port)\n    # for the mesh gateway from.\n    # Can be set to either: `Service`, `NodeIP`, `NodeName` or `Static`.\n    #\n    # - `Service` - Determine the address based on the service type.\n    #\n    #   - If `service.type=LoadBalancer` use the external IP or hostname of\n    #     the service. Use the port set by `service.port`.\n    #\n    #   - If `service.type=NodePort` use the Node IP. The port will be set to\n    #     `service.nodePort` so `service.nodePort` cannot be null.\n    #\n    #   - If `service.type=ClusterIP` use the `ClusterIP`. The port will be set to\n    #     `service.port`.\n    #\n    #   - `service.type=ExternalName` is not supported.\n    #\n    # - `NodeIP` - The node IP as provided by the Kubernetes downward API.\n    #\n    # - `NodeName` - The name of the node as provided by the Kubernetes downward\n    #   API. This is useful if the node names are DNS entries that\n    #   are routable from other datacenters.\n    #\n    # - `Static` - Use the address hardcoded in `meshGateway.wanAddress.static`.\n    source: \"Service\"\n\n    # Port that gets registered for WAN traffic.\n    # If source is set to \"Service\" then this setting will have no effect.\n    # See the documentation for source as to which port will be used in that\n    # case.\n    port: 443\n\n    # If source is set to \"Static\" then this value will be used as the WAN\n    # address of the mesh gateways. This is useful if you've configured a\n    # DNS entry to point to your mesh gateways.\n    static: \"\"\n\n  # The service option configures the Service that fronts the Gateway Deployment.\n  service:\n    # Whether to create a Service or not.\n    enabled: true\n\n    # Type of service, ex. LoadBalancer, ClusterIP.\n    type: LoadBalancer\n\n    # Port that the service will be exposed on.\n    # The targetPort will be set to meshGateway.containerPort.\n    port: 443\n\n    # Optionally hardcode the nodePort of the service if using a NodePort service.\n    # If not set and using a NodePort service, Kubernetes will automatically assign\n    # a port.\n    # @type: integer\n    nodePort: null\n\n    # Annotations to apply to the mesh gateway service.\n    #\n    # Example:\n    #\n    # ```yaml\n    # annotations: |\n    #   'annotation-key': annotation-value\n    # ```\n    # @type: string\n    annotations: null\n\n    # Optional YAML string that will be appended to the Service spec.\n    # @type: string\n    additionalSpec: null\n\n  # If set to true, gateway Pods will run on the host network.\n  hostNetwork: false\n\n  # dnsPolicy to use.\n  # @type: string\n  dnsPolicy: null\n\n  # Consul service name for the mesh gateways.\n  # Cannot be set to anything other than \"mesh-gateway\" if\n  # global.acls.manageSystemACLs is true since the ACL token\n  # generated is only for the name 'mesh-gateway'.\n  consulServiceName: \"mesh-gateway\"\n\n  # Port that the gateway will run on inside the container.\n  containerPort: 8443\n\n  # Optional hostPort for the gateway to be exposed on.\n  # This can be used with wanAddress.port and wanAddress.useNodeIP\n  # to expose the gateways directly from the node.\n  # If hostNetwork is true, this must be null or set to the same port as\n  # containerPort.\n  # NOTE: Cannot set to 8500 or 8502 because those are reserved for the Consul\n  # agent.\n  # @type: integer\n  hostPort: null\n\n  # Resource settings for mesh gateway pods.\n  # NOTE: The use of a YAML string is deprecated. Instead, set directly as a\n  # YAML map.\n  # @recurse: false\n  # @type: map\n  resources:\n    requests:\n      memory: \"100Mi\"\n      cpu: \"100m\"\n    limits:\n      memory: \"100Mi\"\n      cpu: \"100m\"\n\n  # Resource settings for the `copy-consul-bin` init container.\n  # @recurse: false\n  # @type: map\n  initCopyConsulContainer:\n    resources:\n      requests:\n        memory: \"25Mi\"\n        cpu: \"50m\"\n      limits:\n        memory: \"150Mi\"\n        cpu: \"50m\"\n\n  # By default, we set an anti-affinity so that two gateway pods won't be\n  # on the same node. NOTE: Gateways require that Consul client agents are\n  # also running on the nodes alongside each gateway pod.\n  affinity: |\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              app: {{ template \"consul.name\" . }}\n              release: \"{{ .Release.Name }}\"\n              component: mesh-gateway\n          topologyKey: kubernetes.io/hostname\n\n  # Optional YAML string to specify tolerations.\n  # @type: string\n  tolerations: null\n\n  # Optional YAML string to specify a nodeSelector config.\n  # @type: string\n  nodeSelector: null\n\n  # Optional priorityClassName.\n  priorityClassName: \"\"\n\n  # Annotations to apply to the mesh gateway deployment.\n  #\n  # Example:\n  #\n  # ```yaml\n  # annotations: |\n  #   'annotation-key': annotation-value\n  # ```\n  # @type: string\n  annotations: null\n\n# Configuration options for ingress gateways. Default values for all\n# ingress gateways are defined in `ingressGateways.defaults`. Any of\n# these values may be overridden in `ingressGateways.gateways` for a\n# specific gateway with the exception of annotations. Annotations will\n# include both the default annotations and any additional ones defined\n# for a specific gateway.\n# Requirements: consul \u003e= 1.8.0 and consul-k8s \u003e= 0.16.0 if using\n# global.acls.manageSystemACLs and consul-k8s \u003e= 0.10.0 if not.\ningressGateways:\n  # Enable ingress gateway deployment. Requires `connectInject.enabled=true`\n  # and `client.enabled=true`.\n  enabled: false\n\n  # Defaults sets default values for all gateway fields. With the exception\n  # of annotations, defining any of these values in the `gateways` list\n  # will override the default values provided here. Annotations will\n  # include both the default annotations and any additional ones defined\n  # for a specific gateway.\n  defaults:\n    # Number of replicas for each ingress gateway defined.\n    replicas: 2\n\n    # The service options configure the Service that fronts the gateway Deployment.\n    service:\n      # Type of service: LoadBalancer, ClusterIP or NodePort. If using NodePort service\n      # type, you must set the desired nodePorts in the `ports` setting below.\n      type: ClusterIP\n\n      # Ports that will be exposed on the service and gateway container. Any\n      # ports defined as ingress listeners on the gateway's Consul configuration\n      # entry should be included here. The first port will be used as part of\n      # the Consul service registration for the gateway and be listed in its\n      # SRV record. If using a NodePort service type, you must specify the\n      # desired nodePort for each exposed port.\n      # @type: array\u003cmap\u003e\n      # @default: [{port: 8080, port: 8443}]\n      # @recurse: false\n      ports:\n        - port: 8080\n          nodePort: null\n        - port: 8443\n          nodePort: null\n\n      # Annotations to apply to the ingress gateway service. Annotations defined\n      # here will be applied to all ingress gateway services in addition to any\n      # service annotations defined for a specific gateway in `ingressGateways.gateways`.\n      #\n      # Example:\n      #\n      # ```yaml\n      # annotations: |\n      #   'annotation-key': annotation-value\n      # ```\n      # @type: string\n      annotations: null\n\n      # Optional YAML string that will be appended to the Service spec.\n      # @type: string\n      additionalSpec: null\n\n    # Resource limits for all ingress gateway pods\n    # @recurse: false\n    # @type: map\n    resources:\n      requests:\n        memory: \"100Mi\"\n        cpu: \"100m\"\n      limits:\n        memory: \"100Mi\"\n        cpu: \"100m\"\n\n    # Resource settings for the `copy-consul-bin` init container.\n    # @recurse: false\n    # @type: map\n    initCopyConsulContainer:\n      resources:\n        requests:\n          memory: \"25Mi\"\n          cpu: \"50m\"\n        limits:\n          memory: \"150Mi\"\n          cpu: \"50m\"\n\n    # By default, we set an anti-affinity so that two of the same gateway pods\n    # won't be on the same node. NOTE: Gateways require that Consul client agents are\n    # also running on the nodes alongside each gateway pod.\n    affinity: |\n      podAntiAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app: {{ template \"consul.name\" . }}\n                release: \"{{ .Release.Name }}\"\n                component: ingress-gateway\n            topologyKey: kubernetes.io/hostname\n\n    # Optional YAML string to specify tolerations.\n    # @type: string\n    tolerations: null\n\n    # Optional YAML string to specify a nodeSelector config.\n    # @type: string\n    nodeSelector: null\n\n    # Optional priorityClassName.\n    priorityClassName: \"\"\n\n    # Annotations to apply to the ingress gateway deployment. Annotations defined\n    # here will be applied to all ingress gateway deployments in addition to any\n    # annotations defined for a specific gateway in `ingressGateways.gateways`.\n    #\n    # Example:\n    #\n    # ```yaml\n    # annotations: |\n    #   \"annotation-key\": 'annotation-value'\n    # ```\n    # @type: string\n    annotations: null\n\n    # [Enterprise Only] `consulNamespace` defines the Consul namespace to register\n    # the gateway into. Requires `global.enableConsulNamespaces` to be true and\n    # Consul Enterprise v1.7+ with a valid Consul Enterprise license.\n    # Note: The Consul namespace MUST exist before the gateway is deployed.\n    consulNamespace: \"default\"\n\n  # Gateways is a list of gateway objects. The only required field for\n  # each is `name`, though they can also contain any of the fields in\n  # `defaults`. Values defined here override the defaults except in the\n  # case of annotations where both will be applied.\n  # @type: array\u003cmap\u003e\n  gateways:\n    - name: ingress-gateway\n\n# Configuration options for terminating gateways. Default values for all\n# terminating gateways are defined in `terminatingGateways.defaults`. Any of\n# these values may be overridden in `terminatingGateways.gateways` for a\n# specific gateway with the exception of annotations. Annotations will\n# include both the default annotations and any additional ones defined\n# for a specific gateway.\n# Requirements: consul \u003e= 1.8.0 and consul-k8s \u003e= 0.16.0 if using\n# global.acls.manageSystemACLs and consul-k8s \u003e= 0.10.0 if not.\nterminatingGateways:\n  # Enable terminating gateway deployment. Requires `connectInject.enabled=true`\n  # and `client.enabled=true`.\n  enabled: false\n\n  # Defaults sets default values for all gateway fields. With the exception\n  # of annotations, defining any of these values in the `gateways` list\n  # will override the default values provided here. Annotations will\n  # include both the default annotations and any additional ones defined\n  # for a specific gateway.\n  defaults:\n    # Number of replicas for each terminating gateway defined.\n    replicas: 2\n\n    # A list of extra volumes to mount. These will be exposed to Consul in the path `/consul/userconfig/\u003cname\u003e/`.\n    #\n    # Example:\n    #\n    # ```yaml\n    # extraVolumes:\n    #   - type: secret\n    #     name: my-secret\n    #     items: # optional items array\n    #       - key: key\n    #         path: path # secret will now mount to /consul/userconfig/my-secret/path\n    # ```\n    # @type: array\u003cmap\u003e\n    extraVolumes: []\n\n    # Resource limits for all terminating gateway pods\n    # @recurse: false\n    # @type: map\n    resources:\n      requests:\n        memory: \"100Mi\"\n        cpu: \"100m\"\n      limits:\n        memory: \"100Mi\"\n        cpu: \"100m\"\n\n    # Resource settings for the `copy-consul-bin` init container.\n    # @recurse: false\n    # @type: map\n    initCopyConsulContainer:\n      resources:\n        requests:\n          memory: \"25Mi\"\n          cpu: \"50m\"\n        limits:\n          memory: \"150Mi\"\n          cpu: \"50m\"\n\n    # By default, we set an anti-affinity so that two of the same gateway pods\n    # won't be on the same node. NOTE: Gateways require that Consul client agents are\n    # also running on the nodes alongside each gateway pod.\n    affinity: |\n      podAntiAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app: {{ template \"consul.name\" . }}\n                release: \"{{ .Release.Name }}\"\n                component: terminating-gateway\n            topologyKey: kubernetes.io/hostname\n\n    # Optional YAML string to specify tolerations.\n    # @type: string\n    tolerations: null\n\n    # Optional YAML string to specify a nodeSelector config.\n    # @type: string\n    nodeSelector: null\n\n    # Optional priorityClassName.\n    # @type: string\n    priorityClassName: \"\"\n\n    # Annotations to apply to the terminating gateway deployment. Annotations defined\n    # here will be applied to all terminating gateway deployments in addition to any\n    # annotations defined for a specific gateway in `terminatingGateways.gateways`.\n    #\n    # Example:\n    #\n    # ```yaml\n    # annotations: |\n    #   'annotation-key': annotation-value\n    # ```\n    # @type: string\n    annotations: null\n\n    # [Enterprise Only] `consulNamespace` defines the Consul namespace to register\n    # the gateway into. Requires `global.enableConsulNamespaces` to be true and\n    # Consul Enterprise v1.7+ with a valid Consul Enterprise license.\n    # Note: The Consul namespace MUST exist before the gateway is deployed.\n    consulNamespace: \"default\"\n\n  # Gateways is a list of gateway objects. The only required field for\n  # each is `name`, though they can also contain any of the fields in\n  # `defaults`. Values defined here override the defaults except in the\n  # case of annotations where both will be applied.\n  # @type: array\u003cmap\u003e\n  gateways:\n    - name: terminating-gateway\n\n# Control whether a test Pod manifest is generated when running helm template.\n# When using helm install, the test Pod is not submitted to the cluster so this\n# is only useful when running helm template.\ntests:\n  enabled: true\n"
            ],
            "verify": false,
            "version": "0.28.0",
            "wait": true
          },
          "private": "bnVsbA==",
          "dependencies": [
            "aws_eks_cluster.demo",
            "aws_eks_node_group.demo",
            "aws_iam_role.demo-node",
            "aws_iam_role_policy_attachment.demo-node-AmazonEC2ContainerRegistryReadOnly",
            "aws_iam_role_policy_attachment.demo-node-AmazonEKSWorkerNodePolicy",
            "aws_iam_role_policy_attachment.demo-node-AmazonEKS_CNI_Policy",
            "aws_subnet.demo"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "flask-app",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "./helm/flask-redis/",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "flask-app",
            "keyring": null,
            "lint": false,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "1.16.0",
                "chart": "flask-redis",
                "name": "flask-app",
                "namespace": "default",
                "revision": 2,
                "values": "{\"env\":{\"flask1\":{\"image\":\"taran26311/flaskapp\",\"image_tag\":\"latest\",\"replicas\":1},\"flask2\":{\"image\":\"taran26311/flaskapp\",\"image_tag\":\"latest\",\"replicas\":2},\"flask_port\":5000,\"redis\":{\"host\":\"redis\",\"port\":6379}}}",
                "version": "0.1.0"
              }
            ],
            "name": "flask-app",
            "namespace": "default",
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": null,
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [
              {
                "name": "env.flask1.image",
                "type": "",
                "value": "taran26311/flaskapp"
              },
              {
                "name": "env.flask1.image_tag",
                "type": "",
                "value": "latest"
              },
              {
                "name": "env.flask1.replicas",
                "type": "",
                "value": "1"
              },
              {
                "name": "env.flask2.image",
                "type": "",
                "value": "taran26311/flaskapp"
              },
              {
                "name": "env.flask2.image_tag",
                "type": "",
                "value": "latest"
              },
              {
                "name": "env.flask2.replicas",
                "type": "",
                "value": "2"
              },
              {
                "name": "env.flask_port",
                "type": "",
                "value": "5000"
              },
              {
                "name": "env.redis.host",
                "type": "",
                "value": "redis"
              },
              {
                "name": "env.redis.port",
                "type": "",
                "value": "6379"
              }
            ],
            "set_sensitive": [],
            "set_string": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "values": [
              "\nenv:\n  flask1:\n    image: taran26311/flaskapp\n    image_tag: latest\n    replicas: 1\n  flask2:\n    image: taran26311/anotherflaskapp\n    image_tag: latest\n    replicas: 2\n  flask_port: 5000\n  redis:\n    host: redis\n    port: 6379"
            ],
            "verify": false,
            "version": "0.1.0",
            "wait": true
          },
          "private": "bnVsbA==",
          "dependencies": [
            "aws_eks_node_group.demo"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "null_resource",
      "name": "kubernetes-login",
      "provider": "provider[\"registry.terraform.io/hashicorp/null\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "5007077851538781926",
            "triggers": null
          },
          "dependencies": [
            "aws_eks_cluster.demo",
            "aws_iam_role.demo-cluster",
            "aws_iam_role_policy_attachment.demo-cluster-AmazonEKSClusterPolicy",
            "aws_iam_role_policy_attachment.demo-cluster-AmazonEKSVPCResourceController",
            "aws_security_group.demo-cluster",
            "aws_subnet.demo",
            "aws_vpc.demo",
            "data.aws_availability_zones.available"
          ]
        }
      ]
    }
  ]
}
